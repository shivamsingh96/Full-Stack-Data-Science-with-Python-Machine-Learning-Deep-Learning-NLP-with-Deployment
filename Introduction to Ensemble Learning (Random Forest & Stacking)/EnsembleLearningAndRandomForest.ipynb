{"cells":[{"cell_type":"markdown","metadata":{"id":"2KgijSJ-hIqK"},"source":["## Ensemble Techniques\n","\n","We regularly come across many game shows on television and you must have noticed an option of “Audience Poll”. Most of the times a contestant goes with the option which has the highest vote from the audience and most of the times they win. We can generalize this in real life as well where taking opinions from a majority of people is much more preferred than the opinion of a single person.\n","Ensemble technique has a similar underlying idea where we aggregate predictions from a group of predictors, which may be classifiers or regressors, and most of the times the prediction is better than the one obtained using a single predictor. Such algorithms are called Ensemble methods and such predictors are called Ensembles.\n","\n","Let’s suppose we have ‘n’ predictors:\n","\n","Z1, Z2, Z3, ......., Zn with a standard deviation of σ\n","\n","Var(z) = σ^2\n","\n","If we use single predictors Z1, Z2, Z3, ......., Zn the variance associated with each will be σ2 but the expected value will be the average of all the predictors.\n","\n","Let’s consider the average of the predictors:\n","\n","µ = (Z1 + Z2 + Z3+.......+ Zn)/n\n","\n","if we use µ as the predictor then the expected value still remains the same but see the variance now:\n","\n","variance(µ) = σ^2/n\n","\n","So, the expected value remained ‘µ’ but variance decreases when we use average of all the predictors.\n","\n","This is why taking mean is preferred over using single predictors.\n","\n","Ensemble methods take multiple small models and combine their predictions to obtain a more powerful predictive power.\n","\n","There are few very popular Ensemble techniques which we will talk about in detail such as Bagging, Boosting, stacking etc.\n","\n","<img src=\"ensemble.PNG\"> \n","\n","image courtsey: Google\n","\n","<img src=\"1.PNG\">                           \n","\n","\n","\n","### Bagging (Bootstrap Aggregation)\n","\n","In real life scenarios we don’t have multiple different training sets on which we can train our model separately and at the end combine their result. Here, bootstrapping comes into picture. Bootstrapping is a technique of sampling different sets of data from a given training set by using replacement. After bootstrapping the training dataset, we train model on all the different sets and aggregate the result. This technique is known as Bootstrap Aggregation or Bagging.\n","\n","Let’s see definition of bagging:\n","\n","Bagging is the type of ensemble technique in which a single training algorithm is used on different subsets of the training data where the subset sampling is done with replacement (bootstrap). \n","Once the algorithm is trained on all the subsets, then bagging makes the prediction by aggregating all the predictions made by the algorithm on different subsets. In case of regression, bagging prediction is simply the mean of all the predictions and in the case of classifier, bagging prediction is the most frequent prediction (majority vote) among all the predictions.\n","\n","Bagging is also known as parallel model since we run all models parallely and combine there results at the end.\n","\n","<img src=\"2.PNG\">        \n","\n","<img src=\"3.PNG\">  \n","\n","image courtsey: Google\n","\n","* Advantages of a Bagging Model\n","\n","1)\tBagging significantly decreases the variance without increasing bias. \n","\n","2)\tBagging methods work so well because of diversity in the training data since the sampling is done by bootstraping.\n","\n","3)\tAlso, if the training set is very huge, it can save computional time by training model on relatively smaller data set and still can increase the accuracy of the model.\n","\n","4) Works well with small datasets as well.\n","\n","* **Disadvantage of a Bagging Model\n","\n","The main disadvantage of Bagging is that it improves the accuracy of the model on the expense of interpretability i.e. if a single tree was being used as the base model, then it would have a more attarctive and easily interpretable diagram, but with use of bagging this interpretability gets lost.\n","\n","## Pasting\n","\n","Pasting is an ensemble technique similar to bagging with the only difference being that there is no replacement done while sampling the training dataset. This causes less diversity in the sampled datasets and data ends up being correlated. That's why bagging is more preffered than pasting in real scenarios.\n","\n","## Out-of-Bag Evaluation\n","\n","In bagging, when different samples are collected, no sample contains all the data but a fraction of the original dataset.\n","There might be some data which are never sampled at all. The remaining data which are not sampled are called out of bag instances. Since the model never trains over these data, they can be used for evaluating the accuracy of the model by using these data for predicition. We do not need validation set or cross validation and can use out of bag instances for that purpose.\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ds0MaEQGhIqP"},"source":["Let's see python implementation of Bagging:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lNR7waeshIqQ"},"outputs":[],"source":["from sklearn.ensemble import BaggingClassifier\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.tree import DecisionTreeClassifier"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FSAijKkIhIqR"},"outputs":[],"source":["from sklearn.datasets import load_breast_cancer\n","dataset = load_breast_cancer()\n","X = dataset.data\n","y = dataset.target"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ctORqsMfhIqR"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=3)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V3M2FAb0hIqR","outputId":"5cff8a12-5629-4842-965d-47cce2f06fe2"},"outputs":[{"data":{"text/plain":["0.916083916083916"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["knn = KNeighborsClassifier(n_neighbors=5)\n","knn.fit(X_train, y_train)\n","knn.score(X_test, y_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QQDY-iErhIqS","outputId":"7519574e-0cb4-4c40-da8a-f9721291705a"},"outputs":[{"data":{"text/plain":["0.9300699300699301"]},"execution_count":28,"metadata":{},"output_type":"execute_result"}],"source":["dt = DecisionTreeClassifier()\n","dt.fit(X_train, y_train)\n","dt.score(X_test, y_test)"]},{"cell_type":"markdown","metadata":{"id":"uV8ADECshIqT"},"source":["let's using bagging over our KNN classifier and see if our score improves:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"82IPI9fZhIqT"},"outputs":[],"source":["bag_knn = BaggingClassifier(KNeighborsClassifier(n_neighbors=5),\n","                            n_estimators=15, max_samples=0.5,\n","                            bootstrap=True, random_state=3,oob_score=True) "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E36HGmTMhIqU"},"outputs":[],"source":["bag_dt = BaggingClassifier(DecisionTreeClassifier(),\n","                            n_estimators=100, max_samples=0.5,\n","                            bootstrap=True, random_state=3,oob_score=True) "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z6F1Lk9ahIqU"},"outputs":[],"source":["#Let's check the out of bag score \n","#bag_knn.score(X_test, y_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j9QRAm3whIqV","outputId":"b4b07b71-5fc7-4da2-d683-7731b536c216"},"outputs":[{"data":{"text/plain":["0.9370629370629371"]},"execution_count":37,"metadata":{},"output_type":"execute_result"}],"source":["bag_knn.fit(X_train, y_train)\n","bag_knn.score(X_test, y_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gsSiwQSChIqV","outputId":"5c69990e-fad4-414e-c134-434716d70ebb"},"outputs":[{"data":{"text/plain":["0.9370629370629371"]},"execution_count":38,"metadata":{},"output_type":"execute_result"}],"source":["bag_dt.fit(X_train, y_train)\n","bag_dt.score(X_test, y_test)"]},{"cell_type":"markdown","metadata":{"id":"Kry7bCLzhIqV"},"source":["Great! our score sginificantly improves with use of bagging.\n","\n","let's not use bootstrap and see the model accuracy! Remember this is \"Pasting\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wWjYAD5ehIqV"},"outputs":[],"source":["pasting_knn = BaggingClassifier(KNeighborsClassifier(n_neighbors=5),\n","                            n_estimators=10, max_samples=0.5,\n","                            bootstrap=False, random_state=3) "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a9dnhGG-hIqV","outputId":"14d40bd5-ed7e-4d2f-dcd1-9caf2e298cfb"},"outputs":[{"data":{"text/plain":["0.9300699300699301"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["pasting_knn.fit(X_train, y_train)\n","pasting_knn.score(X_test, y_test)"]},{"cell_type":"markdown","metadata":{"id":"8eX7NWlhhIqW"},"source":["## Random Forests\n","\n","Decision trees are one of such models which have low bias but high variance. We have studied that decision trees tend to overfit the data. So bagging technique becomes a very good solution for decreasing the variance in a decision tree.\n","Instead of using a bagging model with underlying model as a decision tree, we can also use Random forest which is more convenient and well optimized for decision trees. The main issue with bagging is that there is not much independence among the sampled datasets i.e. there is correlation. The advantage of random forests over bagging models is that the random forests makes a tweak in the working algorithm of bagging model to decrease the correlation in trees.  The idea is to introduce more randomness while creating trees which will help in reducing correlation.\n","\n","Let’s understand how algorithm works for a random forest model:\n","\n","1)\tJust like in bagging, different samples are collected from the training dataset using bootstraping.\n","\n","2)\tOn each sample we train our tree model and we allow the trees to grow with high depths. \n","\n","    Now, the difference with in random forest is how the trees are formed. In bootstraping we allow all the sample data to be used for splitting the nodes but not   with random forests.  When building a decision tree, each time a split is to happen, a random sample of ‘m’ predictors are chosen from the total ‘p’ predictors. Only those ‘m’ predictors are allowed to be used for the split.\n","\n","    Why is that?\n","\n","    Suppose in those ‘p’ predictors, 1 predictor is very strong. Now each sample this predictor will remain the strongest. So, whenever trees will be built for these sampled data, this predictor will be chosen by all the trees for splitting and thus will result in similar kind of tree formation for each bootstrap model. This introduces correaltion in the dataset and averaging correalted dataset results do not lead low variance. That’s why in random forest the choice for selecting node for split is limited and it introduces randomness in the formation of the trees as well.\n","    Most of the predictors are not allowed to be considered for split.\n","    Generally, value of ‘m’ is taken as m ≈√p , where ‘p’ is the number of predictors in the sample.\n","\n","    When m=p , the random forest model becomes bagging model.   \n","              \n","    *This method is also referred as “Feature Sampling”\n","\n","<img src=\"7.PNG\">\n","\n","\n","    The above graph represents the decrease in test classifcation error as we select different     \n","    values  of ‘m’.\n","\n","3)\tOnce the trees are formed, prediction is made by the random forest by aggregating the predictions of all the model.  For regression model, the mean of all the predictions is the final prediction and for classification mode, the mode of all the predictions is considered the final predictions. \n","\n"]},{"cell_type":"markdown","metadata":{"id":"DzaEKSjFhIqW"},"source":["#### Working of a Random Forest Model\n","\n","<img src=\"random_forest.PNG\">\n","\n","From the given dataset different samples are created by bootstrapping and these samples are used to train different decision trees. Once the training is complete, prediction is made using all the different models.\n","\n","\n","#### Predicting Outcome\n","\n","<img src=\"random_forest2.PNG\">\n","\n","Random forest makes the prediction by taking the mode of all the predictions made by all the models, since this is the case of classification. This process is also known as “Majority voting”.\n","We can also use prediction probability to make the final prediction. We can use the predict_proba method, which will predict a probability from 0 to 1 that a given class is the right one for a row. For a problem with output being only 0 and 1, we'll get a matrix with as many rows as there is in the data and 2 columns. predict_proba will return something like this:\n","\n","<img src=\"8.PNG\">\n","\n","Each row corresponds to a prediction. The first column is the probability that the prediction is a 0, the second column is the probability that the prediction is a 1. Each row adds up to 1.\n","\n","If we just take the second column, we get the average value that the classifier would predict for that row. If there's a .9 probability that the correct classification is 1, we can use the .9 as the value the classifier is predicting. This will give us a continuous output in a single vector instead of just 0 or 1.\n","We can then add all of the vectors we get through this method together and divide by the number of vectors to get the mean prediction by all the members of the ensemble. We can then round off to get 0 or 1 predictions.\n","Similarly, in case of regression Random forest makes the prediction by taking the mean of all the predictions made by different models. \n","\n","#### Advantages and Disadvantages of Random Forest:\n","\n","1)\tIt can be used for both regression and classification problems.\n","\n","2)\tSince base model is a tree, handling of missing values is easy.\n","\n","3)\tIt gives very accurate result with very low variance.\n","\n","4)\tResults of a random forest are very hard to interpret in comparison with decision trees.\n","\n","5)\tHigh computational time than other respective models.\n","\n","\n","Random Forest should be used where accuracy is up utmost priority and interpretability is not very important. Also, computational time is less expensive than the desired outcome.\n"]},{"cell_type":"markdown","metadata":{"id":"Nm0fRAARhIqX"},"source":["Let's see the python implementation of Random forest."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"toqbV3qBhIqX","outputId":"ffa573b4-75b3-408c-e685-9880b433c54e"},"outputs":[{"name":"stderr","output_type":"stream","text":["C:\\Users\\Mohit Kashyap\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\six.py:31: DeprecationWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n","  \"(https://pypi.org/project/six/).\", DeprecationWarning)\n"]}],"source":["import pandas as pd\n","from sklearn.tree import DecisionTreeClassifier, export_graphviz\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn import tree\n","from sklearn.model_selection import train_test_split,GridSearchCV\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import accuracy_score, confusion_matrix, roc_curve, roc_auc_score\n","from sklearn.externals.six import StringIO  \n","from IPython.display import Image  \n","from sklearn.tree import export_graphviz\n","import pydotplus"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oA3gUW60hIqX","outputId":"554264a5-2a5d-4d98-fe1d-ea8d8e3b5e87"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>fixed acidity</th>\n","      <th>volatile acidity</th>\n","      <th>citric acid</th>\n","      <th>residual sugar</th>\n","      <th>chlorides</th>\n","      <th>free sulfur dioxide</th>\n","      <th>total sulfur dioxide</th>\n","      <th>density</th>\n","      <th>pH</th>\n","      <th>sulphates</th>\n","      <th>alcohol</th>\n","      <th>quality</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>0</td>\n","      <td>7.4</td>\n","      <td>0.700</td>\n","      <td>0.00</td>\n","      <td>1.9</td>\n","      <td>0.076</td>\n","      <td>11.0</td>\n","      <td>34.0</td>\n","      <td>0.99780</td>\n","      <td>3.51</td>\n","      <td>0.56</td>\n","      <td>9.4</td>\n","      <td>5</td>\n","    </tr>\n","    <tr>\n","      <td>1</td>\n","      <td>7.8</td>\n","      <td>0.880</td>\n","      <td>0.00</td>\n","      <td>2.6</td>\n","      <td>0.098</td>\n","      <td>25.0</td>\n","      <td>67.0</td>\n","      <td>0.99680</td>\n","      <td>3.20</td>\n","      <td>0.68</td>\n","      <td>9.8</td>\n","      <td>5</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>7.8</td>\n","      <td>0.760</td>\n","      <td>0.04</td>\n","      <td>2.3</td>\n","      <td>0.092</td>\n","      <td>15.0</td>\n","      <td>54.0</td>\n","      <td>0.99700</td>\n","      <td>3.26</td>\n","      <td>0.65</td>\n","      <td>9.8</td>\n","      <td>5</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>11.2</td>\n","      <td>0.280</td>\n","      <td>0.56</td>\n","      <td>1.9</td>\n","      <td>0.075</td>\n","      <td>17.0</td>\n","      <td>60.0</td>\n","      <td>0.99800</td>\n","      <td>3.16</td>\n","      <td>0.58</td>\n","      <td>9.8</td>\n","      <td>6</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>7.4</td>\n","      <td>0.700</td>\n","      <td>0.00</td>\n","      <td>1.9</td>\n","      <td>0.076</td>\n","      <td>11.0</td>\n","      <td>34.0</td>\n","      <td>0.99780</td>\n","      <td>3.51</td>\n","      <td>0.56</td>\n","      <td>9.4</td>\n","      <td>5</td>\n","    </tr>\n","    <tr>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <td>1594</td>\n","      <td>6.2</td>\n","      <td>0.600</td>\n","      <td>0.08</td>\n","      <td>2.0</td>\n","      <td>0.090</td>\n","      <td>32.0</td>\n","      <td>44.0</td>\n","      <td>0.99490</td>\n","      <td>3.45</td>\n","      <td>0.58</td>\n","      <td>10.5</td>\n","      <td>5</td>\n","    </tr>\n","    <tr>\n","      <td>1595</td>\n","      <td>5.9</td>\n","      <td>0.550</td>\n","      <td>0.10</td>\n","      <td>2.2</td>\n","      <td>0.062</td>\n","      <td>39.0</td>\n","      <td>51.0</td>\n","      <td>0.99512</td>\n","      <td>3.52</td>\n","      <td>0.76</td>\n","      <td>11.2</td>\n","      <td>6</td>\n","    </tr>\n","    <tr>\n","      <td>1596</td>\n","      <td>6.3</td>\n","      <td>0.510</td>\n","      <td>0.13</td>\n","      <td>2.3</td>\n","      <td>0.076</td>\n","      <td>29.0</td>\n","      <td>40.0</td>\n","      <td>0.99574</td>\n","      <td>3.42</td>\n","      <td>0.75</td>\n","      <td>11.0</td>\n","      <td>6</td>\n","    </tr>\n","    <tr>\n","      <td>1597</td>\n","      <td>5.9</td>\n","      <td>0.645</td>\n","      <td>0.12</td>\n","      <td>2.0</td>\n","      <td>0.075</td>\n","      <td>32.0</td>\n","      <td>44.0</td>\n","      <td>0.99547</td>\n","      <td>3.57</td>\n","      <td>0.71</td>\n","      <td>10.2</td>\n","      <td>5</td>\n","    </tr>\n","    <tr>\n","      <td>1598</td>\n","      <td>6.0</td>\n","      <td>0.310</td>\n","      <td>0.47</td>\n","      <td>3.6</td>\n","      <td>0.067</td>\n","      <td>18.0</td>\n","      <td>42.0</td>\n","      <td>0.99549</td>\n","      <td>3.39</td>\n","      <td>0.66</td>\n","      <td>11.0</td>\n","      <td>6</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>1599 rows × 12 columns</p>\n","</div>"],"text/plain":["      fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n","0               7.4             0.700         0.00             1.9      0.076   \n","1               7.8             0.880         0.00             2.6      0.098   \n","2               7.8             0.760         0.04             2.3      0.092   \n","3              11.2             0.280         0.56             1.9      0.075   \n","4               7.4             0.700         0.00             1.9      0.076   \n","...             ...               ...          ...             ...        ...   \n","1594            6.2             0.600         0.08             2.0      0.090   \n","1595            5.9             0.550         0.10             2.2      0.062   \n","1596            6.3             0.510         0.13             2.3      0.076   \n","1597            5.9             0.645         0.12             2.0      0.075   \n","1598            6.0             0.310         0.47             3.6      0.067   \n","\n","      free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n","0                    11.0                  34.0  0.99780  3.51       0.56   \n","1                    25.0                  67.0  0.99680  3.20       0.68   \n","2                    15.0                  54.0  0.99700  3.26       0.65   \n","3                    17.0                  60.0  0.99800  3.16       0.58   \n","4                    11.0                  34.0  0.99780  3.51       0.56   \n","...                   ...                   ...      ...   ...        ...   \n","1594                 32.0                  44.0  0.99490  3.45       0.58   \n","1595                 39.0                  51.0  0.99512  3.52       0.76   \n","1596                 29.0                  40.0  0.99574  3.42       0.75   \n","1597                 32.0                  44.0  0.99547  3.57       0.71   \n","1598                 18.0                  42.0  0.99549  3.39       0.66   \n","\n","      alcohol  quality  \n","0         9.4        5  \n","1         9.8        5  \n","2         9.8        5  \n","3         9.8        6  \n","4         9.4        5  \n","...       ...      ...  \n","1594     10.5        5  \n","1595     11.2        6  \n","1596     11.0        6  \n","1597     10.2        5  \n","1598     11.0        6  \n","\n","[1599 rows x 12 columns]"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["data = pd.read_csv(\"winequality_red.csv\")\n","data"]},{"cell_type":"markdown","metadata":{"id":"2P4XV7XmhIqX"},"source":["The data set consists following Input variables :\n","1 - fixed acidity  2 - volatile acidity  3 - citric acid  4 - residual sugar  5 - chlorides  6 - free sulfur dioxide\n","\n","7 - total sulfur dioxide  8 - density  9 - pH   10 - sulphates   11 - alcohol\n","\n","and the Output variable gives the quality of th wine based on the input variables: \n","\n","12 - quality (score between 0 and 10)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mio62ZOkhIqX","outputId":"54319176-5b13-428f-8799-bffb210bfe3b"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>fixed acidity</th>\n","      <th>volatile acidity</th>\n","      <th>citric acid</th>\n","      <th>residual sugar</th>\n","      <th>chlorides</th>\n","      <th>free sulfur dioxide</th>\n","      <th>total sulfur dioxide</th>\n","      <th>density</th>\n","      <th>pH</th>\n","      <th>sulphates</th>\n","      <th>alcohol</th>\n","      <th>quality</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>count</td>\n","      <td>1599.000000</td>\n","      <td>1599.000000</td>\n","      <td>1599.000000</td>\n","      <td>1599.000000</td>\n","      <td>1599.000000</td>\n","      <td>1599.000000</td>\n","      <td>1599.000000</td>\n","      <td>1599.000000</td>\n","      <td>1599.000000</td>\n","      <td>1599.000000</td>\n","      <td>1599.000000</td>\n","      <td>1599.000000</td>\n","    </tr>\n","    <tr>\n","      <td>mean</td>\n","      <td>8.319637</td>\n","      <td>0.527821</td>\n","      <td>0.270976</td>\n","      <td>2.538806</td>\n","      <td>0.087467</td>\n","      <td>15.874922</td>\n","      <td>46.467792</td>\n","      <td>0.996747</td>\n","      <td>3.311113</td>\n","      <td>0.658149</td>\n","      <td>10.422983</td>\n","      <td>5.636023</td>\n","    </tr>\n","    <tr>\n","      <td>std</td>\n","      <td>1.741096</td>\n","      <td>0.179060</td>\n","      <td>0.194801</td>\n","      <td>1.409928</td>\n","      <td>0.047065</td>\n","      <td>10.460157</td>\n","      <td>32.895324</td>\n","      <td>0.001887</td>\n","      <td>0.154386</td>\n","      <td>0.169507</td>\n","      <td>1.065668</td>\n","      <td>0.807569</td>\n","    </tr>\n","    <tr>\n","      <td>min</td>\n","      <td>4.600000</td>\n","      <td>0.120000</td>\n","      <td>0.000000</td>\n","      <td>0.900000</td>\n","      <td>0.012000</td>\n","      <td>1.000000</td>\n","      <td>6.000000</td>\n","      <td>0.990070</td>\n","      <td>2.740000</td>\n","      <td>0.330000</td>\n","      <td>8.400000</td>\n","      <td>3.000000</td>\n","    </tr>\n","    <tr>\n","      <td>25%</td>\n","      <td>7.100000</td>\n","      <td>0.390000</td>\n","      <td>0.090000</td>\n","      <td>1.900000</td>\n","      <td>0.070000</td>\n","      <td>7.000000</td>\n","      <td>22.000000</td>\n","      <td>0.995600</td>\n","      <td>3.210000</td>\n","      <td>0.550000</td>\n","      <td>9.500000</td>\n","      <td>5.000000</td>\n","    </tr>\n","    <tr>\n","      <td>50%</td>\n","      <td>7.900000</td>\n","      <td>0.520000</td>\n","      <td>0.260000</td>\n","      <td>2.200000</td>\n","      <td>0.079000</td>\n","      <td>14.000000</td>\n","      <td>38.000000</td>\n","      <td>0.996750</td>\n","      <td>3.310000</td>\n","      <td>0.620000</td>\n","      <td>10.200000</td>\n","      <td>6.000000</td>\n","    </tr>\n","    <tr>\n","      <td>75%</td>\n","      <td>9.200000</td>\n","      <td>0.640000</td>\n","      <td>0.420000</td>\n","      <td>2.600000</td>\n","      <td>0.090000</td>\n","      <td>21.000000</td>\n","      <td>62.000000</td>\n","      <td>0.997835</td>\n","      <td>3.400000</td>\n","      <td>0.730000</td>\n","      <td>11.100000</td>\n","      <td>6.000000</td>\n","    </tr>\n","    <tr>\n","      <td>max</td>\n","      <td>15.900000</td>\n","      <td>1.580000</td>\n","      <td>1.000000</td>\n","      <td>15.500000</td>\n","      <td>0.611000</td>\n","      <td>72.000000</td>\n","      <td>289.000000</td>\n","      <td>1.003690</td>\n","      <td>4.010000</td>\n","      <td>2.000000</td>\n","      <td>14.900000</td>\n","      <td>8.000000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["       fixed acidity  volatile acidity  citric acid  residual sugar  \\\n","count    1599.000000       1599.000000  1599.000000     1599.000000   \n","mean        8.319637          0.527821     0.270976        2.538806   \n","std         1.741096          0.179060     0.194801        1.409928   \n","min         4.600000          0.120000     0.000000        0.900000   \n","25%         7.100000          0.390000     0.090000        1.900000   \n","50%         7.900000          0.520000     0.260000        2.200000   \n","75%         9.200000          0.640000     0.420000        2.600000   \n","max        15.900000          1.580000     1.000000       15.500000   \n","\n","         chlorides  free sulfur dioxide  total sulfur dioxide      density  \\\n","count  1599.000000          1599.000000           1599.000000  1599.000000   \n","mean      0.087467            15.874922             46.467792     0.996747   \n","std       0.047065            10.460157             32.895324     0.001887   \n","min       0.012000             1.000000              6.000000     0.990070   \n","25%       0.070000             7.000000             22.000000     0.995600   \n","50%       0.079000            14.000000             38.000000     0.996750   \n","75%       0.090000            21.000000             62.000000     0.997835   \n","max       0.611000            72.000000            289.000000     1.003690   \n","\n","                pH    sulphates      alcohol      quality  \n","count  1599.000000  1599.000000  1599.000000  1599.000000  \n","mean      3.311113     0.658149    10.422983     5.636023  \n","std       0.154386     0.169507     1.065668     0.807569  \n","min       2.740000     0.330000     8.400000     3.000000  \n","25%       3.210000     0.550000     9.500000     5.000000  \n","50%       3.310000     0.620000    10.200000     6.000000  \n","75%       3.400000     0.730000    11.100000     6.000000  \n","max       4.010000     2.000000    14.900000     8.000000  "]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["data.describe()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iQzZnSrPhIqX"},"outputs":[],"source":["X = data.drop(columns = 'quality')\n","y = data['quality']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Tf8_KAPOhIqY"},"outputs":[],"source":["x_train,x_test,y_train,y_test = train_test_split(X,y,test_size = 0.30, random_state= 355)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jKKjKgf7hIqY","outputId":"63301fce-8283-475f-803f-5d00cbb69b0f"},"outputs":[{"data":{"text/plain":["DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n","                       max_features=None, max_leaf_nodes=None,\n","                       min_impurity_decrease=0.0, min_impurity_split=None,\n","                       min_samples_leaf=1, min_samples_split=2,\n","                       min_weight_fraction_leaf=0.0, presort=False,\n","                       random_state=None, splitter='best')"]},"execution_count":39,"metadata":{},"output_type":"execute_result"}],"source":["#let's first visualize the tree on the data without doing any pre processing\n","clf = DecisionTreeClassifier( min_samples_split= 2)\n","clf.fit(x_train,y_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mgf9MgkEhIqY","outputId":"47a115a1-039e-453b-e975-df0fbe39b65f"},"outputs":[{"data":{"text/plain":["0.61875"]},"execution_count":42,"metadata":{},"output_type":"execute_result"}],"source":["# accuracy of our classification tree\n","clf.score(x_test,y_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w2OIF79mhIqY","outputId":"391cb2ea-192e-4200-b28d-e470a8ce41a6"},"outputs":[{"data":{"text/plain":["DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=24,\n","                       max_features=None, max_leaf_nodes=None,\n","                       min_impurity_decrease=0.0, min_impurity_split=None,\n","                       min_samples_leaf=1, min_samples_split=2,\n","                       min_weight_fraction_leaf=0.0, presort=False,\n","                       random_state=None, splitter='best')"]},"execution_count":49,"metadata":{},"output_type":"execute_result"}],"source":["#let's first visualize the tree on the data without doing any pre processing\n","clf2 = DecisionTreeClassifier(criterion = 'entropy', max_depth =24, min_samples_leaf= 1)\n","clf2.fit(x_train,y_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DjdnnxnBhIqY","outputId":"0708aa3b-e849-412d-c70f-14f3357ebb56"},"outputs":[{"data":{"text/plain":["0.6104166666666667"]},"execution_count":50,"metadata":{},"output_type":"execute_result"}],"source":["clf2.score(x_test,y_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IUqPU1LzhIqY"},"outputs":[],"source":["rand_clf = RandomForestClassifier(random_state=6)"]},{"cell_type":"markdown","metadata":{"id":"7vJDg02FhIqY"},"source":["Random state, if given none then score will vary everytime you run the RandomForestClassifier. If we asssign a value to it, then result will remain constant."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FTriIYrrhIqY","outputId":"ff2aed43-2d62-4da8-fe46-504e94881466"},"outputs":[{"name":"stderr","output_type":"stream","text":["C:\\Users\\Mohit Kashyap\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n","  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"]},{"data":{"text/plain":["RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n","                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n","                       min_impurity_decrease=0.0, min_impurity_split=None,\n","                       min_samples_leaf=1, min_samples_split=2,\n","                       min_weight_fraction_leaf=0.0, n_estimators=10,\n","                       n_jobs=None, oob_score=False, random_state=6, verbose=0,\n","                       warm_start=False)"]},"execution_count":121,"metadata":{},"output_type":"execute_result"}],"source":["rand_clf.fit(x_train,y_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f2iFYlVphIqY","outputId":"d8b5a29f-8531-4857-c5ba-804dc7cd1da5"},"outputs":[{"data":{"text/plain":["0.64375"]},"execution_count":122,"metadata":{},"output_type":"execute_result"}],"source":["rand_clf.score(x_test,y_test)"]},{"cell_type":"markdown","metadata":{"id":"lRYw7m7FhIqZ"},"source":["We can see that two individual decision trees have both less score than a single random forest classifier. \n","\n","So, using random forest classifier has increased the predicitive power of our model. \n","\n","Great, let's do some hyperparameter tuning and see if we can increase our accuracy more.\n","\n","Random forest hyperparameters are a combination of best hyperparameters of both decision tree and Bagging classifier."]},{"cell_type":"markdown","metadata":{"id":"ZFET2R8LhIqZ"},"source":["* Hyperparameters of Decision tree: \n","\n","class_weight=None, criterion='entropy', max_depth=24,max_features=None, max_leaf_nodes=None,\n","                       min_impurity_decrease=0.0, min_impurity_split=None,\n","                       min_samples_leaf=1, min_samples_split=2,\n","                       min_weight_fraction_leaf=0.0, presort=False,\n","                       random_state=None, splitter='best'\n","                    \n","* Hyperparameters of Bagging classifier:\n","\n","base_estimator=None, bootstrap=True, bootstrap_features=False,\n","                  max_features=1.0, max_samples=1.0, n_estimators=10,\n","                  n_jobs=None, oob_score=False, random_state=None, verbose=0,\n","                  warm_start=False\n","                  \n","* Hyperparameters of Random forest classifier:\n","\n","bootstrap=True, class_weight=None, criterion='gini',\n","                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n","                       min_impurity_decrease=0.0, min_impurity_split=None,\n","                       min_samples_leaf=1, min_samples_split=2,\n","                       min_weight_fraction_leaf=0.0, n_estimators=10,\n","                       n_jobs=None, oob_score=False, random_state=None,\n","                       verbose=0, warm_start=False"]},{"cell_type":"markdown","metadata":{"id":"WczDSQqehIqZ"},"source":["Let's now try to tune some hyperparameters using the GridSearchCV algorithm.\n","We have studied about CrossValidation in upcoming lecture. \n","\n","GridSearchCV is a method used to tune our hyperparameters. We can pass different values of hyperparameters as parameters for grid search.\n","It does a exhaustive generation of combination of different parameters passed.\n","Using cross validation score, Grid Search returns the combination of hyperparameters for which the model is performing the best. \n","\n","Note that it is common that a small subset of those parameters can have a large impact on the predictive or computation performance of the model while others can be left to their default values. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OqYChuIshIqZ"},"outputs":[],"source":["# we are tuning three hyperparameters right now, we are passing the different values for both parameters\n","grid_param = {\n","    \"n_estimators\" : [90,100,115,130],\n","    'criterion': ['gini', 'entropy'],\n","    'max_depth' : range(2,20,1),\n","    'min_samples_leaf' : range(1,10,1),\n","    'min_samples_split': range(2,10,1),\n","    'max_features' : ['auto','log2']\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1tZTpxqOhIqZ"},"outputs":[],"source":["grid_search = GridSearchCV(estimator=rand_clf,param_grid=grid_param,cv=5,n_jobs =-1,verbose = 3)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZHqpj4jzhIqZ","outputId":"e195fafe-e639-4d52-c31f-a8496ad232f4"},"outputs":[{"name":"stdout","output_type":"stream","text":["Fitting 5 folds for each of 20736 candidates, totalling 103680 fits\n"]},{"name":"stderr","output_type":"stream","text":["[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n","[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:    2.5s\n","[Parallel(n_jobs=-1)]: Done 112 tasks      | elapsed:    4.3s\n","[Parallel(n_jobs=-1)]: Done 272 tasks      | elapsed:    7.4s\n","[Parallel(n_jobs=-1)]: Done 496 tasks      | elapsed:   11.6s\n","[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:   17.5s\n","[Parallel(n_jobs=-1)]: Done 1136 tasks      | elapsed:   24.4s\n","[Parallel(n_jobs=-1)]: Done 1552 tasks      | elapsed:   34.0s\n","[Parallel(n_jobs=-1)]: Done 2032 tasks      | elapsed:   44.4s\n","[Parallel(n_jobs=-1)]: Done 2576 tasks      | elapsed:   55.4s\n","[Parallel(n_jobs=-1)]: Done 3184 tasks      | elapsed:  1.1min\n","[Parallel(n_jobs=-1)]: Done 3856 tasks      | elapsed:  1.4min\n","[Parallel(n_jobs=-1)]: Done 4592 tasks      | elapsed:  1.7min\n","[Parallel(n_jobs=-1)]: Done 5392 tasks      | elapsed:  2.0min\n","[Parallel(n_jobs=-1)]: Done 6256 tasks      | elapsed:  2.3min\n","[Parallel(n_jobs=-1)]: Done 7184 tasks      | elapsed:  2.7min\n","[Parallel(n_jobs=-1)]: Done 8176 tasks      | elapsed:  3.1min\n","[Parallel(n_jobs=-1)]: Done 9232 tasks      | elapsed:  3.5min\n","[Parallel(n_jobs=-1)]: Done 10352 tasks      | elapsed:  4.0min\n","[Parallel(n_jobs=-1)]: Done 11536 tasks      | elapsed:  4.5min\n","[Parallel(n_jobs=-1)]: Done 12784 tasks      | elapsed:  5.1min\n","[Parallel(n_jobs=-1)]: Done 14096 tasks      | elapsed:  5.7min\n","[Parallel(n_jobs=-1)]: Done 15472 tasks      | elapsed:  6.4min\n","[Parallel(n_jobs=-1)]: Done 16912 tasks      | elapsed:  7.1min\n","[Parallel(n_jobs=-1)]: Done 18416 tasks      | elapsed:  7.9min\n","[Parallel(n_jobs=-1)]: Done 19984 tasks      | elapsed:  8.7min\n","[Parallel(n_jobs=-1)]: Done 21616 tasks      | elapsed:  9.5min\n","[Parallel(n_jobs=-1)]: Done 23312 tasks      | elapsed: 10.5min\n","[Parallel(n_jobs=-1)]: Done 25072 tasks      | elapsed: 11.4min\n","[Parallel(n_jobs=-1)]: Done 26896 tasks      | elapsed: 12.4min\n","[Parallel(n_jobs=-1)]: Done 28784 tasks      | elapsed: 13.4min\n","[Parallel(n_jobs=-1)]: Done 30736 tasks      | elapsed: 14.5min\n","[Parallel(n_jobs=-1)]: Done 32752 tasks      | elapsed: 15.6min\n","[Parallel(n_jobs=-1)]: Done 34832 tasks      | elapsed: 16.7min\n","[Parallel(n_jobs=-1)]: Done 36976 tasks      | elapsed: 17.9min\n","[Parallel(n_jobs=-1)]: Done 39184 tasks      | elapsed: 19.1min\n","[Parallel(n_jobs=-1)]: Done 41456 tasks      | elapsed: 20.3min\n","[Parallel(n_jobs=-1)]: Done 43792 tasks      | elapsed: 21.6min\n","[Parallel(n_jobs=-1)]: Done 46192 tasks      | elapsed: 22.9min\n","[Parallel(n_jobs=-1)]: Done 48656 tasks      | elapsed: 24.2min\n","[Parallel(n_jobs=-1)]: Done 51184 tasks      | elapsed: 25.7min\n","[Parallel(n_jobs=-1)]: Done 53776 tasks      | elapsed: 26.9min\n","[Parallel(n_jobs=-1)]: Done 56432 tasks      | elapsed: 28.1min\n","[Parallel(n_jobs=-1)]: Done 59152 tasks      | elapsed: 29.4min\n","[Parallel(n_jobs=-1)]: Done 61936 tasks      | elapsed: 31.0min\n","[Parallel(n_jobs=-1)]: Done 64784 tasks      | elapsed: 32.6min\n","[Parallel(n_jobs=-1)]: Done 67696 tasks      | elapsed: 34.5min\n","[Parallel(n_jobs=-1)]: Done 70672 tasks      | elapsed: 36.6min\n","[Parallel(n_jobs=-1)]: Done 73712 tasks      | elapsed: 38.7min\n","[Parallel(n_jobs=-1)]: Done 76816 tasks      | elapsed: 41.0min\n","[Parallel(n_jobs=-1)]: Done 79984 tasks      | elapsed: 43.3min\n","[Parallel(n_jobs=-1)]: Done 83216 tasks      | elapsed: 45.7min\n","[Parallel(n_jobs=-1)]: Done 86512 tasks      | elapsed: 48.1min\n","[Parallel(n_jobs=-1)]: Done 89872 tasks      | elapsed: 50.6min\n","[Parallel(n_jobs=-1)]: Done 93296 tasks      | elapsed: 53.4min\n","[Parallel(n_jobs=-1)]: Done 96784 tasks      | elapsed: 56.4min\n","[Parallel(n_jobs=-1)]: Done 100336 tasks      | elapsed: 59.7min\n","[Parallel(n_jobs=-1)]: Done 103680 out of 103680 | elapsed: 62.5min finished\n","C:\\Users\\Mohit Kashyap\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n","  DeprecationWarning)\n"]},{"data":{"text/plain":["GridSearchCV(cv=5, error_score='raise-deprecating',\n","             estimator=RandomForestClassifier(bootstrap=True, class_weight=None,\n","                                              criterion='gini', max_depth=None,\n","                                              max_features='auto',\n","                                              max_leaf_nodes=None,\n","                                              min_impurity_decrease=0.0,\n","                                              min_impurity_split=None,\n","                                              min_samples_leaf=1,\n","                                              min_samples_split=2,\n","                                              min_weight_fraction_leaf=0.0,\n","                                              n_estimators=10, n_jobs=None,\n","                                              oob_score=False,\n","                                              random_state=None, verbose=0,\n","                                              warm_start=False),\n","             iid='warn', n_jobs=-1,\n","             param_grid={'criterion': ['gini', 'entropy'],\n","                         'max_depth': range(2, 20),\n","                         'max_features': ['auto', 'log2'],\n","                         'min_samples_leaf': range(1, 10),\n","                         'min_samples_split': range(2, 10),\n","                         'n_estimators': [90, 100, 115, 130]},\n","             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n","             scoring=None, verbose=3)"]},"execution_count":89,"metadata":{},"output_type":"execute_result"}],"source":["grid_search.fit(x_train,y_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ek7x_jUwhIqZ","outputId":"49cda9e7-9c85-4b15-e410-ee11d21d6296"},"outputs":[{"data":{"text/plain":["{'criterion': 'entropy',\n"," 'max_depth': 12,\n"," 'max_features': 'log2',\n"," 'min_samples_leaf': 1,\n"," 'min_samples_split': 5,\n"," 'n_estimators': 90}"]},"execution_count":126,"metadata":{},"output_type":"execute_result"}],"source":["#let's see the best parameters as per our grid search\n","grid_search.best_params_"]},{"cell_type":"markdown","metadata":{"id":"u5TUXEFKhIqZ"},"source":["We will pass these parameters into our random forest classifier."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aT2K2_SJhIqZ"},"outputs":[],"source":["rand_clf = RandomForestClassifier(criterion= 'entropy',\n"," max_depth = 12,\n"," max_features = 'log2',\n"," min_samples_leaf = 1,\n"," min_samples_split= 5,\n"," n_estimators = 90,random_state=6)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O7V8OYW0hIqZ","outputId":"592b2810-c3a6-48e5-98d6-3546940846a0"},"outputs":[{"data":{"text/plain":["RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n","                       max_depth=12, max_features='log2', max_leaf_nodes=None,\n","                       min_impurity_decrease=0.0, min_impurity_split=None,\n","                       min_samples_leaf=1, min_samples_split=5,\n","                       min_weight_fraction_leaf=0.0, n_estimators=90,\n","                       n_jobs=None, oob_score=False, random_state=6, verbose=0,\n","                       warm_start=False)"]},"execution_count":128,"metadata":{},"output_type":"execute_result"}],"source":["rand_clf.fit(x_train,y_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uQCgQj4qhIqa","outputId":"8c3c0b5b-9874-4de6-f8bc-e21a4da54344"},"outputs":[{"data":{"text/plain":["0.6604166666666667"]},"execution_count":129,"metadata":{},"output_type":"execute_result"}],"source":["rand_clf.score(x_test,y_test)"]},{"cell_type":"markdown","metadata":{"id":"Cnov4CIQhIqa"},"source":["Great! Our accuracy has increased by 2% after using the best parameters for GridsearchCV.\n","\n","Let's do some more tweak in the hyper parameters and try gridSearch on it."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iiXICOmlhIqa"},"outputs":[],"source":["# we are tuning three hyperparameters right now, we are passing the different values for both parameters\n","grid_param = {\n","    \"n_estimators\" : [90,100,115],\n","    'criterion': ['gini', 'entropy'],\n","    'min_samples_leaf' : [1,2,3,4,5],\n","    'min_samples_split': [4,5,6,7,8],\n","    'max_features' : ['auto','log2']\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eXquTu1ChIqa"},"outputs":[],"source":["grid_search = GridSearchCV(estimator=rand_clf,param_grid=grid_param,cv=5,n_jobs =-1,verbose = 3)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"twebHLD6hIqa","outputId":"832c76db-a13c-4d04-ed6d-b36693be3c9e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Fitting 5 folds for each of 300 candidates, totalling 1500 fits\n"]},{"name":"stderr","output_type":"stream","text":["[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n","[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:    7.2s\n","[Parallel(n_jobs=-1)]: Done 112 tasks      | elapsed:   12.1s\n","[Parallel(n_jobs=-1)]: Done 272 tasks      | elapsed:   19.5s\n","[Parallel(n_jobs=-1)]: Done 496 tasks      | elapsed:   30.0s\n","[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:   44.2s\n","[Parallel(n_jobs=-1)]: Done 1136 tasks      | elapsed:  1.1min\n","[Parallel(n_jobs=-1)]: Done 1500 out of 1500 | elapsed:  1.5min finished\n","C:\\Users\\Mohit Kashyap\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n","  DeprecationWarning)\n"]},{"data":{"text/plain":["GridSearchCV(cv=5, error_score='raise-deprecating',\n","             estimator=RandomForestClassifier(bootstrap=True, class_weight=None,\n","                                              criterion='gini', max_depth=12,\n","                                              max_features='log2',\n","                                              max_leaf_nodes=None,\n","                                              min_impurity_decrease=0.0,\n","                                              min_impurity_split=None,\n","                                              min_samples_leaf=1,\n","                                              min_samples_split=5,\n","                                              min_weight_fraction_leaf=0.0,\n","                                              n_estimators=100, n_jobs=None,\n","                                              oob_score=False, random_state=6,\n","                                              verbose=0, warm_start=False),\n","             iid='warn', n_jobs=-1,\n","             param_grid={'criterion': ['gini', 'entropy'],\n","                         'max_features': ['auto', 'log2'],\n","                         'min_samples_leaf': [1, 2, 3, 4, 5],\n","                         'min_samples_split': [4, 5, 6, 7, 8],\n","                         'n_estimators': [90, 100, 115]},\n","             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n","             scoring=None, verbose=3)"]},"execution_count":141,"metadata":{},"output_type":"execute_result"}],"source":["grid_search.fit(x_train,y_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vY-53mgShIqa","outputId":"7f4ac425-0bd5-4661-f39e-5b472ae7cb3e"},"outputs":[{"data":{"text/plain":["{'criterion': 'entropy',\n"," 'max_features': 'auto',\n"," 'min_samples_leaf': 1,\n"," 'min_samples_split': 4,\n"," 'n_estimators': 115}"]},"execution_count":143,"metadata":{},"output_type":"execute_result"}],"source":["#let's see the best parameters as per our grid search\n","grid_search.best_params_"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sNPpGY_AhIqa"},"outputs":[],"source":["rand_clf = RandomForestClassifier(criterion= 'entropy',\n"," max_features = 'auto',\n"," min_samples_leaf = 1,\n"," min_samples_split= 4,\n"," n_estimators = 115,random_state=6)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JI-6x4s8hIqa","outputId":"d5cd793c-9731-4c8b-8f76-cf2287332131"},"outputs":[{"data":{"text/plain":["RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n","                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n","                       min_impurity_decrease=0.0, min_impurity_split=None,\n","                       min_samples_leaf=1, min_samples_split=4,\n","                       min_weight_fraction_leaf=0.0, n_estimators=115,\n","                       n_jobs=None, oob_score=False, random_state=6, verbose=0,\n","                       warm_start=False)"]},"execution_count":145,"metadata":{},"output_type":"execute_result"}],"source":["rand_clf.fit(x_train,y_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"alFhhExzhIqa","outputId":"a6724ebc-6d37-47a8-fa81-f401cb9e657e"},"outputs":[{"data":{"text/plain":["0.6729166666666667"]},"execution_count":146,"metadata":{},"output_type":"execute_result"}],"source":["rand_clf.score(x_test,y_test)"]},{"cell_type":"markdown","metadata":{"id":"Klpw3p6NhIqa"},"source":["Our accuracy has improved and score is better than the last grid search. So, we can say that giving all the hyperparameters in the gridSearch \n","doesn't gurantee the best result. We have to do hit and trial with parameters to get the perfect score.\n","\n","You are welcome to try tweaking the parameters more and try an improve the accuracy more.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RV3RYW1BhIqa"},"outputs":[],"source":["# let's save the model\n","\n","import pickle\n","\n","with open('D:\\ineuron_materials_ipynb\\iNeuron\\EnsembleLearning_And_RandomForest'+ '/modelForPrediction.sav', 'wb') as f:\n","    pickle.dump(rand_clf,f)\n"]},{"cell_type":"markdown","metadata":{"id":"x_FR6CruhIqa"},"source":["## Stacking (Stacked Generalization)\n","\n","Stacking is a type of ensemble technique which combines the predictions of two or more models, also called base models, and use the combination as the input for a new model (meta-model) i.e. a new model is trained on the predictions of the base models. \n"," \n","\n","<img src=\"stacking.png\" width=\"700\">\n","\n","\n","Suppose you have a classification problem and you can use several models like logistic regression, SVM, KNN, Random forest etc. The idea is to use few models like KNN, SVM as the base model and make predictions using these models. Now the predictions made by these models are used as an input feature for Random forest to train on and give prediction.\n","\n","\n","Stacking, just like other ensemble techniques, tries to improve the accuracy of a model by using predictions of not so good models and then using those predictions as an input feature for a better model.\n","\n","Stacking can be multilevel e.g. using base models as level 1 then passing the predictions into another set of sub-base models at level 2 and so on. Then at the end using meta-model/models which take predictions of the last sub base models as input and does prediction.\n","\n","Let's understand more by looking at the steps involved for stacking:\n","\n","*\tSplit the dataset into a training set and a holdout set. We can use k-fold validation for seleting different set of validation sets.\n","\n","   Generally, we do a 50-50 split of the training set and the hold out set. \n","   \n","   training set = x1,y1\n","   hold out set = x2, y2\n","\n","*\tSplit the training set again into training and test dataset e.g. x1_train, y1_train, x1_test, y1_test\n","\n","*\tTrain all the base models on training set  x1_train, y1_train.\n","\n","*\tAfter training is done, get the predictions of all the base models on the validation set x2. \n","\n","*\tStack all these predictions together (you can also take an average of all the predictions or probability prediction) as it will be used as input feature for the meta_model.\n","\n","*\tAgain, get the prediction for all the base models on the test set i.e. x1_test \n","\n","*\tAgain, stack all these predictions together (you can also take an average of all the predictions or probability prediction) as it will be used as the prediction dataset for the meta_model.\n","\n","*\tUse the stacked data from step 5 as the input feature for meta_model and validation set y2 as the target variable and train the model on these data.\n","\n","*\tOnce, the training is done check the accuracy of meta_model by using data from step 7 for prediction and y1_test for evaluation.\n"]},{"cell_type":"markdown","metadata":{"id":"Nrx3Bl2ThIqb"},"source":["Although, there is no libraries available in Sklearn for stacking, it can still be implemented.\n","\n","Let's understand more about stacking with python implementation:\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lRT7MyiBhIqb","outputId":"9da4f69b-dfce-41ff-b334-697cbad321d5"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>fixed acidity</th>\n","      <th>volatile acidity</th>\n","      <th>citric acid</th>\n","      <th>residual sugar</th>\n","      <th>chlorides</th>\n","      <th>free sulfur dioxide</th>\n","      <th>total sulfur dioxide</th>\n","      <th>density</th>\n","      <th>pH</th>\n","      <th>sulphates</th>\n","      <th>alcohol</th>\n","      <th>quality</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>count</td>\n","      <td>1599.000000</td>\n","      <td>1599.000000</td>\n","      <td>1599.000000</td>\n","      <td>1599.000000</td>\n","      <td>1599.000000</td>\n","      <td>1599.000000</td>\n","      <td>1599.000000</td>\n","      <td>1599.000000</td>\n","      <td>1599.000000</td>\n","      <td>1599.000000</td>\n","      <td>1599.000000</td>\n","      <td>1599.000000</td>\n","    </tr>\n","    <tr>\n","      <td>mean</td>\n","      <td>8.319637</td>\n","      <td>0.527821</td>\n","      <td>0.270976</td>\n","      <td>2.538806</td>\n","      <td>0.087467</td>\n","      <td>15.874922</td>\n","      <td>46.467792</td>\n","      <td>0.996747</td>\n","      <td>3.311113</td>\n","      <td>0.658149</td>\n","      <td>10.422983</td>\n","      <td>5.636023</td>\n","    </tr>\n","    <tr>\n","      <td>std</td>\n","      <td>1.741096</td>\n","      <td>0.179060</td>\n","      <td>0.194801</td>\n","      <td>1.409928</td>\n","      <td>0.047065</td>\n","      <td>10.460157</td>\n","      <td>32.895324</td>\n","      <td>0.001887</td>\n","      <td>0.154386</td>\n","      <td>0.169507</td>\n","      <td>1.065668</td>\n","      <td>0.807569</td>\n","    </tr>\n","    <tr>\n","      <td>min</td>\n","      <td>4.600000</td>\n","      <td>0.120000</td>\n","      <td>0.000000</td>\n","      <td>0.900000</td>\n","      <td>0.012000</td>\n","      <td>1.000000</td>\n","      <td>6.000000</td>\n","      <td>0.990070</td>\n","      <td>2.740000</td>\n","      <td>0.330000</td>\n","      <td>8.400000</td>\n","      <td>3.000000</td>\n","    </tr>\n","    <tr>\n","      <td>25%</td>\n","      <td>7.100000</td>\n","      <td>0.390000</td>\n","      <td>0.090000</td>\n","      <td>1.900000</td>\n","      <td>0.070000</td>\n","      <td>7.000000</td>\n","      <td>22.000000</td>\n","      <td>0.995600</td>\n","      <td>3.210000</td>\n","      <td>0.550000</td>\n","      <td>9.500000</td>\n","      <td>5.000000</td>\n","    </tr>\n","    <tr>\n","      <td>50%</td>\n","      <td>7.900000</td>\n","      <td>0.520000</td>\n","      <td>0.260000</td>\n","      <td>2.200000</td>\n","      <td>0.079000</td>\n","      <td>14.000000</td>\n","      <td>38.000000</td>\n","      <td>0.996750</td>\n","      <td>3.310000</td>\n","      <td>0.620000</td>\n","      <td>10.200000</td>\n","      <td>6.000000</td>\n","    </tr>\n","    <tr>\n","      <td>75%</td>\n","      <td>9.200000</td>\n","      <td>0.640000</td>\n","      <td>0.420000</td>\n","      <td>2.600000</td>\n","      <td>0.090000</td>\n","      <td>21.000000</td>\n","      <td>62.000000</td>\n","      <td>0.997835</td>\n","      <td>3.400000</td>\n","      <td>0.730000</td>\n","      <td>11.100000</td>\n","      <td>6.000000</td>\n","    </tr>\n","    <tr>\n","      <td>max</td>\n","      <td>15.900000</td>\n","      <td>1.580000</td>\n","      <td>1.000000</td>\n","      <td>15.500000</td>\n","      <td>0.611000</td>\n","      <td>72.000000</td>\n","      <td>289.000000</td>\n","      <td>1.003690</td>\n","      <td>4.010000</td>\n","      <td>2.000000</td>\n","      <td>14.900000</td>\n","      <td>8.000000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["       fixed acidity  volatile acidity  citric acid  residual sugar  \\\n","count    1599.000000       1599.000000  1599.000000     1599.000000   \n","mean        8.319637          0.527821     0.270976        2.538806   \n","std         1.741096          0.179060     0.194801        1.409928   \n","min         4.600000          0.120000     0.000000        0.900000   \n","25%         7.100000          0.390000     0.090000        1.900000   \n","50%         7.900000          0.520000     0.260000        2.200000   \n","75%         9.200000          0.640000     0.420000        2.600000   \n","max        15.900000          1.580000     1.000000       15.500000   \n","\n","         chlorides  free sulfur dioxide  total sulfur dioxide      density  \\\n","count  1599.000000          1599.000000           1599.000000  1599.000000   \n","mean      0.087467            15.874922             46.467792     0.996747   \n","std       0.047065            10.460157             32.895324     0.001887   \n","min       0.012000             1.000000              6.000000     0.990070   \n","25%       0.070000             7.000000             22.000000     0.995600   \n","50%       0.079000            14.000000             38.000000     0.996750   \n","75%       0.090000            21.000000             62.000000     0.997835   \n","max       0.611000            72.000000            289.000000     1.003690   \n","\n","                pH    sulphates      alcohol      quality  \n","count  1599.000000  1599.000000  1599.000000  1599.000000  \n","mean      3.311113     0.658149    10.422983     5.636023  \n","std       0.154386     0.169507     1.065668     0.807569  \n","min       2.740000     0.330000     8.400000     3.000000  \n","25%       3.210000     0.550000     9.500000     5.000000  \n","50%       3.310000     0.620000    10.200000     6.000000  \n","75%       3.400000     0.730000    11.100000     6.000000  \n","max       4.010000     2.000000    14.900000     8.000000  "]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["# Let's use the same dataset used above for Random forest classification and try to improve the accuracy more using stacking.\n","data.describe()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v3w_9tgkhIqb"},"outputs":[],"source":["import pandas as pd\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.svm import SVC\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn import tree\n","from sklearn.model_selection import train_test_split\n","import numpy as np\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3KYVEODfhIqb","outputId":"21b224d2-d100-4aa6-a717-881dc65f3642"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Pregnancies</th>\n","      <th>Glucose</th>\n","      <th>BloodPressure</th>\n","      <th>SkinThickness</th>\n","      <th>Insulin</th>\n","      <th>BMI</th>\n","      <th>DiabetesPedigreeFunction</th>\n","      <th>Age</th>\n","      <th>Outcome</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>0</td>\n","      <td>6</td>\n","      <td>148</td>\n","      <td>72</td>\n","      <td>35</td>\n","      <td>0</td>\n","      <td>33.6</td>\n","      <td>0.627</td>\n","      <td>50</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>85</td>\n","      <td>66</td>\n","      <td>29</td>\n","      <td>0</td>\n","      <td>26.6</td>\n","      <td>0.351</td>\n","      <td>31</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>8</td>\n","      <td>183</td>\n","      <td>64</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>23.3</td>\n","      <td>0.672</td>\n","      <td>32</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>89</td>\n","      <td>66</td>\n","      <td>23</td>\n","      <td>94</td>\n","      <td>28.1</td>\n","      <td>0.167</td>\n","      <td>21</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>137</td>\n","      <td>40</td>\n","      <td>35</td>\n","      <td>168</td>\n","      <td>43.1</td>\n","      <td>2.288</td>\n","      <td>33</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <td>763</td>\n","      <td>10</td>\n","      <td>101</td>\n","      <td>76</td>\n","      <td>48</td>\n","      <td>180</td>\n","      <td>32.9</td>\n","      <td>0.171</td>\n","      <td>63</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <td>764</td>\n","      <td>2</td>\n","      <td>122</td>\n","      <td>70</td>\n","      <td>27</td>\n","      <td>0</td>\n","      <td>36.8</td>\n","      <td>0.340</td>\n","      <td>27</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <td>765</td>\n","      <td>5</td>\n","      <td>121</td>\n","      <td>72</td>\n","      <td>23</td>\n","      <td>112</td>\n","      <td>26.2</td>\n","      <td>0.245</td>\n","      <td>30</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <td>766</td>\n","      <td>1</td>\n","      <td>126</td>\n","      <td>60</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>30.1</td>\n","      <td>0.349</td>\n","      <td>47</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <td>767</td>\n","      <td>1</td>\n","      <td>93</td>\n","      <td>70</td>\n","      <td>31</td>\n","      <td>0</td>\n","      <td>30.4</td>\n","      <td>0.315</td>\n","      <td>23</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>768 rows × 9 columns</p>\n","</div>"],"text/plain":["     Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n","0              6      148             72             35        0  33.6   \n","1              1       85             66             29        0  26.6   \n","2              8      183             64              0        0  23.3   \n","3              1       89             66             23       94  28.1   \n","4              0      137             40             35      168  43.1   \n","..           ...      ...            ...            ...      ...   ...   \n","763           10      101             76             48      180  32.9   \n","764            2      122             70             27        0  36.8   \n","765            5      121             72             23      112  26.2   \n","766            1      126             60              0        0  30.1   \n","767            1       93             70             31        0  30.4   \n","\n","     DiabetesPedigreeFunction  Age  Outcome  \n","0                       0.627   50        1  \n","1                       0.351   31        0  \n","2                       0.672   32        1  \n","3                       0.167   21        0  \n","4                       2.288   33        1  \n","..                        ...  ...      ...  \n","763                     0.171   63        0  \n","764                     0.340   27        0  \n","765                     0.245   30        0  \n","766                     0.349   47        1  \n","767                     0.315   23        0  \n","\n","[768 rows x 9 columns]"]},"execution_count":95,"metadata":{},"output_type":"execute_result"}],"source":["data = pd.read_csv(\"diabetes.csv\")\n","data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h9lTNxX8hIqb"},"outputs":[],"source":["X = data.drop(columns = 'Outcome')\n","y = data['Outcome']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MbATv68ghIqb"},"outputs":[],"source":["# let's divide our dataset into training set and hold out set by 50%\n","train,val_train,test,val_test = train_test_split(X,y,test_size=0.5, random_state= 355)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YyjW7x0ahIqb"},"outputs":[],"source":["# let's split the training set again into training and test dataset \n","x_train,x_test,y_train,y_test =  train_test_split(train,test,test_size=0.2, random_state= 355)"]},{"cell_type":"markdown","metadata":{"id":"8Z6GPdgWhIqb"},"source":["We will use KNN and SVM algorithm as our base models.\n","\n","Let's fit both of the models first on the x_train and y_train data."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i3KEcVKghIqb","outputId":"bbca1710-fc5e-409e-f495-c1ef0109fe03"},"outputs":[{"data":{"text/plain":["KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n","                     metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n","                     weights='uniform')"]},"execution_count":100,"metadata":{},"output_type":"execute_result"}],"source":["knn = KNeighborsClassifier()\n","\n","knn.fit(x_train,y_train)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yFnbdMwShIqb","outputId":"54137a39-b50d-4d15-d6c2-244e9f26a120"},"outputs":[{"data":{"text/plain":["0.7402597402597403"]},"execution_count":101,"metadata":{},"output_type":"execute_result"}],"source":["knn.score(x_test,y_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zuN8GzjQhIqc","outputId":"e066a6ad-c843-4d31-f092-83888aab3aee"},"outputs":[{"name":"stderr","output_type":"stream","text":["C:\\Users\\Mohit Kashyap\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n","  \"avoid this warning.\", FutureWarning)\n"]},{"data":{"text/plain":["SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n","    decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n","    kernel='rbf', max_iter=-1, probability=False, random_state=None,\n","    shrinking=True, tol=0.001, verbose=False)"]},"execution_count":102,"metadata":{},"output_type":"execute_result"}],"source":["# rand_clf = RandomForestClassifier()\n","\n","# rand_clf.fit(x_train,y_train)\n","\n","\n","svm = SVC()\n","svm.fit(x_train,y_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"drLTfCfkhIqc","outputId":"599470c2-c309-4f99-9e97-921f3821815f"},"outputs":[{"data":{"text/plain":["0.6493506493506493"]},"execution_count":103,"metadata":{},"output_type":"execute_result"}],"source":["#rand_clf.score(x_test,y_test)\n","\n","svm.score(x_test,y_test)"]},{"cell_type":"markdown","metadata":{"id":"ycz8cGhZhIqc"},"source":["Let's get the predictions of all the base models on the validation set val_train."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uToJw_bohIqc"},"outputs":[],"source":["predict_val1 = knn.predict(val_train)\n","predict_val2 = svm.predict(val_train)\n","#predict_val2 = rand_clf.predict(val_train)"]},{"cell_type":"markdown","metadata":{"id":"QeOoengUhIqc"},"source":["Let's stack the prediciton values for validation set together as \"predict_val\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y8nP0yE_hIqc","outputId":"bf21f730-5708-49fa-e9c4-d6a3c3ce7bae"},"outputs":[{"data":{"text/plain":["array([[0, 0],\n","       [0, 0],\n","       [1, 0],\n","       [0, 0],\n","       [0, 0],\n","       [0, 0],\n","       [0, 0],\n","       [0, 0],\n","       [0, 0],\n","       [1, 0],\n","       [1, 0],\n","       [0, 0],\n","       [0, 0],\n","       [0, 0],\n","       [1, 0],\n","       [0, 0],\n","       [0, 0],\n","       [0, 0],\n","       [0, 0],\n","       [0, 0],\n","       [0, 0],\n","       [1, 0],\n","       [1, 0],\n","       [0, 0],\n","       [1, 0],\n","       [0, 0],\n","       [0, 0],\n","       [0, 0],\n","       [0, 0],\n","       [0, 0],\n","       [1, 0],\n","       [0, 0],\n","       [1, 0],\n","       [1, 0],\n","       [0, 0],\n","       [0, 0],\n","       [0, 0],\n","       [0, 0],\n","       [0, 0],\n","       [0, 0],\n","       [0, 0],\n","       [0, 0],\n","       [1, 0],\n","       [1, 0],\n","       [0, 0],\n","       [0, 0],\n","       [0, 0],\n","       [0, 0],\n","       [0, 0],\n","       [1, 0],\n","       [1, 0],\n","       [1, 0],\n","       [0, 0],\n","       [0, 0],\n","       [0, 0],\n","       [0, 0],\n","       [0, 0],\n","       [0, 0],\n","       [1, 0],\n","       [1, 0],\n","       [0, 0],\n","       [0, 0],\n","       [1, 0],\n","       [1, 0],\n","       [1, 0],\n","       [1, 0],\n","       [1, 0],\n","       [0, 0],\n","       [0, 0],\n","       [0, 0],\n","       [0, 0],\n","       [0, 0],\n","       [1, 0],\n","       [1, 0],\n","       [0, 0],\n","       [0, 0],\n","       [0, 0],\n","       [1, 0],\n","       [1, 0],\n","       [0, 0],\n","       [0, 0],\n","       [0, 0],\n","       [0, 0],\n","       [0, 0],\n","       [1, 0],\n","       [0, 0],\n","       [1, 0],\n","       [1, 0],\n","       [0, 0],\n","       [0, 0],\n","       [1, 0],\n","       [0, 0],\n","       [0, 0],\n","       [0, 0],\n","       [0, 0],\n","       [0, 0],\n","       [0, 0],\n","       [0, 0],\n","       [0, 0],\n","       [0, 0],\n","       [0, 0],\n","       [1, 0],\n","       [0, 0],\n","       [0, 0],\n","       [0, 0],\n","       [1, 0],\n","       [0, 0],\n","       [0, 0],\n","       [0, 0],\n","       [0, 0],\n","       [1, 0],\n","       [0, 0],\n","       [0, 0],\n","       [0, 0],\n","       [1, 0],\n","       [0, 0],\n","       [0, 0],\n","       [1, 0],\n","       [0, 0],\n","       [0, 0],\n","       [1, 0],\n","       [0, 0],\n","       [0, 0],\n","       [0, 0],\n","       [1, 0],\n","       [1, 0],\n","       [0, 0],\n","       [0, 0],\n","       [0, 0],\n","       [0, 0],\n","       [1, 0],\n","       [1, 0],\n","       [0, 0],\n","       [0, 0],\n","       [0, 0],\n","       [0, 0],\n","       [1, 0],\n","       [0, 0],\n","       [1, 0],\n","       [0, 0],\n","       [0, 0],\n","       [0, 0],\n","       [0, 0],\n","       [1, 0],\n","       [0, 0],\n","       [0, 0],\n","       [1, 0],\n","       [0, 0],\n","       [0, 0],\n","       [0, 0],\n","       [0, 0],\n","       [1, 0],\n","       [1, 0],\n","       [0, 0],\n","       [0, 0],\n","       [1, 0],\n","       [1, 0],\n","       [0, 0],\n","       [0, 0],\n","       [0, 0],\n","       [1, 0],\n","       [0, 0],\n","       [1, 0],\n","       [1, 0],\n","       [0, 0],\n","       [0, 0],\n","       [1, 0],\n","       [1, 0],\n","       [1, 0],\n","       [0, 0],\n","       [0, 0],\n","       [0, 0],\n","       [0, 0],\n","       [0, 0],\n","       [0, 0],\n","       [1, 0],\n","       [0, 0],\n","       [0, 0],\n","       [1, 0],\n","       [0, 0],\n","       [0, 0],\n","       [0, 0],\n","       [0, 0],\n","       [0, 0],\n","       [0, 0],\n","       [0, 0],\n","       [0, 0],\n","       [0, 0],\n","       [1, 0],\n","       [0, 0],\n","       [1, 0],\n","       [0, 0],\n","       [0, 0],\n","       [0, 0],\n","       [1, 0],\n","       [0, 0],\n","       [1, 0],\n","       [0, 0],\n","       [0, 0],\n","       [0, 0],\n","       [1, 0],\n","       [0, 0],\n","       [0, 0],\n","       [0, 0],\n","       [0, 0],\n","       [0, 0],\n","       [1, 0],\n","       [1, 0],\n","       [0, 0],\n","       [0, 0],\n","       [0, 0],\n","       [0, 0],\n","       [0, 0],\n","       [0, 0],\n","       [0, 0],\n","       [0, 0],\n","       [1, 0],\n","       [0, 0],\n","       [0, 0],\n","       [0, 0],\n","       [0, 0],\n","       [0, 0],\n","       [0, 0],\n","       [0, 0],\n","       [0, 0],\n","       [0, 0],\n","       [0, 0],\n","       [0, 0],\n","       [1, 0],\n","       [0, 0],\n","       [1, 0],\n","       [1, 0],\n","       [0, 0],\n","       [1, 0],\n","       [0, 0],\n","       [0, 0],\n","       [0, 0],\n","       [1, 0],\n","       [0, 0],\n","       [1, 0],\n","       [0, 0],\n","       [1, 0],\n","       [0, 0],\n","       [1, 0],\n","       [0, 0],\n","       [0, 0],\n","       [1, 0],\n","       [0, 0],\n","       [0, 0],\n","       [0, 0],\n","       [0, 0],\n","       [1, 0],\n","       [0, 0],\n","       [0, 0],\n","       [0, 0],\n","       [0, 0],\n","       [0, 0],\n","       [1, 0],\n","       [0, 0],\n","       [0, 0],\n","       [0, 0],\n","       [0, 0],\n","       [1, 0],\n","       [1, 0],\n","       [0, 0],\n","       [0, 0],\n","       [1, 0],\n","       [0, 0],\n","       [1, 0],\n","       [0, 0],\n","       [0, 0],\n","       [0, 0],\n","       [0, 0],\n","       [1, 0],\n","       [0, 0],\n","       [0, 0],\n","       [0, 0],\n","       [0, 0],\n","       [0, 0],\n","       [0, 0],\n","       [0, 0],\n","       [1, 0],\n","       [0, 0],\n","       [0, 0],\n","       [0, 0],\n","       [1, 0],\n","       [0, 0],\n","       [0, 0],\n","       [0, 0],\n","       [0, 0],\n","       [0, 0],\n","       [1, 0],\n","       [0, 0],\n","       [0, 0],\n","       [0, 0],\n","       [1, 0],\n","       [1, 0],\n","       [0, 0],\n","       [1, 0],\n","       [0, 0],\n","       [1, 0],\n","       [1, 0],\n","       [1, 0],\n","       [1, 0],\n","       [0, 0],\n","       [0, 0],\n","       [0, 0],\n","       [0, 0],\n","       [1, 0],\n","       [0, 0],\n","       [0, 0],\n","       [0, 0],\n","       [0, 0],\n","       [1, 0],\n","       [0, 0],\n","       [1, 0],\n","       [1, 0],\n","       [0, 0],\n","       [0, 0],\n","       [0, 0],\n","       [1, 0],\n","       [1, 0],\n","       [0, 0],\n","       [0, 0],\n","       [0, 0],\n","       [0, 0],\n","       [1, 0],\n","       [1, 0],\n","       [0, 0],\n","       [1, 0],\n","       [0, 0],\n","       [0, 0],\n","       [0, 0],\n","       [0, 0],\n","       [0, 0],\n","       [1, 0],\n","       [1, 0],\n","       [0, 0],\n","       [0, 0],\n","       [1, 0],\n","       [0, 0],\n","       [0, 0],\n","       [0, 0],\n","       [0, 0],\n","       [0, 0],\n","       [1, 0],\n","       [1, 0],\n","       [0, 0],\n","       [0, 0],\n","       [1, 0],\n","       [0, 0],\n","       [1, 0],\n","       [0, 0],\n","       [1, 0],\n","       [1, 0],\n","       [0, 0],\n","       [0, 0],\n","       [0, 0],\n","       [0, 0],\n","       [0, 0],\n","       [0, 0],\n","       [1, 0],\n","       [1, 0],\n","       [0, 0],\n","       [0, 0],\n","       [0, 0],\n","       [1, 0],\n","       [0, 0],\n","       [1, 0],\n","       [0, 0],\n","       [1, 0],\n","       [1, 0],\n","       [0, 0],\n","       [0, 0],\n","       [0, 0],\n","       [0, 0],\n","       [0, 0],\n","       [0, 0],\n","       [1, 0],\n","       [1, 0],\n","       [1, 0],\n","       [1, 0],\n","       [0, 0],\n","       [1, 0]], dtype=int64)"]},"execution_count":105,"metadata":{},"output_type":"execute_result"}],"source":["predict_val = np.column_stack((predict_val1,predict_val2))\n","predict_val"]},{"cell_type":"markdown","metadata":{"id":"2B9KmclmhIqc"},"source":["Let's get the prediction for all the base models on the test set  x_test"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J3-atGtGhIqc"},"outputs":[],"source":["predict_test1 = knn.predict(x_test)\n","predict_test2 = svm.predict(x_test)\n","#predict_test2 = rand_clf.predict(x_test)"]},{"cell_type":"markdown","metadata":{"id":"a2v32HRahIqc"},"source":["Let's stack the prediciton values for validation set together as \"predict_test\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F_Kr9uI5hIqc","outputId":"d771e1d8-a7b7-4977-cfed-12e3082b4bf5"},"outputs":[{"data":{"text/plain":["array([[1, 0],\n","       [0, 0],\n","       [1, 0],\n","       [1, 0],\n","       [0, 0],\n","       [1, 0],\n","       [1, 0],\n","       [1, 0],\n","       [0, 0],\n","       [0, 0],\n","       [0, 0],\n","       [0, 0],\n","       [0, 0],\n","       [0, 0],\n","       [0, 0],\n","       [0, 0],\n","       [0, 0],\n","       [0, 0],\n","       [0, 0],\n","       [1, 0],\n","       [0, 0],\n","       [1, 0],\n","       [0, 0],\n","       [0, 0],\n","       [0, 0],\n","       [0, 0],\n","       [0, 0],\n","       [1, 0],\n","       [1, 0],\n","       [0, 0],\n","       [0, 0],\n","       [1, 0],\n","       [0, 0],\n","       [1, 0],\n","       [1, 0],\n","       [0, 0],\n","       [0, 0],\n","       [0, 0],\n","       [0, 0],\n","       [1, 0],\n","       [0, 0],\n","       [0, 0],\n","       [0, 0],\n","       [1, 0],\n","       [1, 0],\n","       [0, 0],\n","       [1, 0],\n","       [1, 0],\n","       [1, 0],\n","       [0, 0],\n","       [0, 0],\n","       [1, 0],\n","       [0, 0],\n","       [0, 0],\n","       [1, 0],\n","       [0, 0],\n","       [0, 0],\n","       [0, 0],\n","       [0, 0],\n","       [0, 0],\n","       [0, 0],\n","       [1, 0],\n","       [0, 0],\n","       [0, 0],\n","       [1, 0],\n","       [0, 0],\n","       [1, 0],\n","       [1, 0],\n","       [1, 0],\n","       [0, 0],\n","       [0, 0],\n","       [1, 0],\n","       [0, 0],\n","       [0, 0],\n","       [1, 0],\n","       [0, 0],\n","       [1, 0]], dtype=int64)"]},"execution_count":107,"metadata":{},"output_type":"execute_result"}],"source":["predict_test = np.column_stack((predict_test1,predict_test2))\n","predict_test"]},{"cell_type":"markdown","metadata":{"id":"ukG-vwFyhIqc"},"source":["Let's use the Use the stacked data \"predict_val\" and val_test as the input feature for meta_model i.e. randomforest classifier"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6Q8ccwSEhIqc"},"outputs":[],"source":["# svm = SVC()\n","#svm.fit(predict_val,val_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iET3Bf8DhIqd"},"outputs":[],"source":["#svm.score(predict_test,y_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y_evZuA0hIqd","outputId":"38c97fbe-242c-4550-ae83-47afc8667830"},"outputs":[{"name":"stderr","output_type":"stream","text":["C:\\Users\\Mohit Kashyap\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n","  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"]},{"data":{"text/plain":["RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n","                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n","                       min_impurity_decrease=0.0, min_impurity_split=None,\n","                       min_samples_leaf=1, min_samples_split=2,\n","                       min_weight_fraction_leaf=0.0, n_estimators=10,\n","                       n_jobs=None, oob_score=False, random_state=None,\n","                       verbose=0, warm_start=False)"]},"execution_count":110,"metadata":{},"output_type":"execute_result"}],"source":["rand_clf = RandomForestClassifier()\n","\n","rand_clf.fit(predict_val,val_test)"]},{"cell_type":"markdown","metadata":{"id":"6-qTa2X8hIqd"},"source":["Let's check the accuracy of our meta_model using predict_test and y_test."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fW5RDSq1hIqd","outputId":"674e74ef-541b-4774-b24a-159759685bde"},"outputs":[{"data":{"text/plain":["0.7402597402597403"]},"execution_count":111,"metadata":{},"output_type":"execute_result"}],"source":["rand_clf.score(predict_test,y_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"82CxaU2chIqd"},"outputs":[],"source":["# we are tuning three hyperparameters right now, we are passing the different values for both parameters\n","grid_param = {\n","    \"n_estimators\" : [90,100,115],\n","    'criterion': ['gini', 'entropy'],\n","    'min_samples_leaf' : [1,2,3,4,5],\n","    'min_samples_split': [4,5,6,7,8],\n","    'max_features' : ['auto','log2']\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-rmNu4I9hIqd"},"outputs":[],"source":["grid_search = GridSearchCV(estimator=rand_clf,param_grid=grid_param,cv=5,n_jobs =-1,verbose = 3)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FI8jj9hqhIqd","outputId":"a20de7c3-b454-40e1-c50d-4b419ee34708"},"outputs":[{"name":"stdout","output_type":"stream","text":["Fitting 5 folds for each of 300 candidates, totalling 1500 fits\n"]},{"name":"stderr","output_type":"stream","text":["[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n","[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:    2.5s\n","[Parallel(n_jobs=-1)]: Done 136 tasks      | elapsed:    4.1s\n","[Parallel(n_jobs=-1)]: Done 456 tasks      | elapsed:    8.4s\n","[Parallel(n_jobs=-1)]: Done 904 tasks      | elapsed:   14.3s\n","[Parallel(n_jobs=-1)]: Done 1500 out of 1500 | elapsed:   22.2s finished\n"]},{"data":{"text/plain":["GridSearchCV(cv=5, error_score='raise-deprecating',\n","             estimator=RandomForestClassifier(bootstrap=True, class_weight=None,\n","                                              criterion='gini', max_depth=None,\n","                                              max_features='auto',\n","                                              max_leaf_nodes=None,\n","                                              min_impurity_decrease=0.0,\n","                                              min_impurity_split=None,\n","                                              min_samples_leaf=1,\n","                                              min_samples_split=2,\n","                                              min_weight_fraction_leaf=0.0,\n","                                              n_estimators=10, n_jobs=None,\n","                                              oob_score=False,\n","                                              random_state=None, verbose=0,\n","                                              warm_start=False),\n","             iid='warn', n_jobs=-1,\n","             param_grid={'criterion': ['gini', 'entropy'],\n","                         'max_features': ['auto', 'log2'],\n","                         'min_samples_leaf': [1, 2, 3, 4, 5],\n","                         'min_samples_split': [4, 5, 6, 7, 8],\n","                         'n_estimators': [90, 100, 115]},\n","             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n","             scoring=None, verbose=3)"]},"execution_count":114,"metadata":{},"output_type":"execute_result"}],"source":["grid_search.fit(predict_val,val_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TsKDeju_hIqd","outputId":"f6e9f19d-c0e8-4915-d5d8-7f9ab08dcd79"},"outputs":[{"data":{"text/plain":["{'criterion': 'gini',\n"," 'max_features': 'auto',\n"," 'min_samples_leaf': 1,\n"," 'min_samples_split': 4,\n"," 'n_estimators': 90}"]},"execution_count":115,"metadata":{},"output_type":"execute_result"}],"source":["grid_search.best_params_"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rwNosuLchIqd"},"outputs":[],"source":["rand_clf = RandomForestClassifier( criterion='gini',max_features = 'auto',min_samples_leaf =1,min_samples_split= 4,n_estimators =90)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rkcLIvMEhIqd","outputId":"a4b34488-1198-48a6-fff0-237f6ff49499"},"outputs":[{"data":{"text/plain":["RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n","                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n","                       min_impurity_decrease=0.0, min_impurity_split=None,\n","                       min_samples_leaf=1, min_samples_split=4,\n","                       min_weight_fraction_leaf=0.0, n_estimators=90,\n","                       n_jobs=None, oob_score=False, random_state=None,\n","                       verbose=0, warm_start=False)"]},"execution_count":117,"metadata":{},"output_type":"execute_result"}],"source":["rand_clf.fit(predict_val,val_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X3Mq9ajehIqd","outputId":"453805ae-e261-4c63-9dd8-c1cc02b830c6"},"outputs":[{"data":{"text/plain":["0.7402597402597403"]},"execution_count":118,"metadata":{},"output_type":"execute_result"}],"source":["rand_clf.score(predict_test,y_test)"]},{"cell_type":"markdown","metadata":{"id":"rsolwslYhIqd"},"source":["Let's see the cloud deployment for Random forest algorithm."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3KSCEeGThIqe"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}