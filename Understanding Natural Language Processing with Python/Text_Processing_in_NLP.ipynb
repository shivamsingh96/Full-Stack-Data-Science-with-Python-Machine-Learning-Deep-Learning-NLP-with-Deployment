{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XbfeYqCSJFm5"
      },
      "source": [
        "Suppose, We're in a situation to create a Sentiment analysis model, we have the dataset available but the problem here is machine will not going to understand the sentences of any languages, we have to clean those dataset by using stopwords, deleting punctuation and many more irrelevant things inside the data and We have to make it upto that level where we can feed those data to our machine or deep learning algorithms from that we can get some output with it.\n",
        "\n",
        "We are assuming you have a knowledge of Python, and if not, nothing to worry, we will give you some overview of it.\n",
        "\n",
        "## 1.Python String\n",
        "\n",
        "A String is like a sequence of Characters.\n",
        "\n",
        "A character is just a symbol.Like, the English language has 26 characters.\n",
        "\n",
        "Computers don't deal with characters,they deal with binary(numbers) only. Even though you seen characters but internally it is stored and manipulated with the combination of 0's and 1's.\n",
        "The conversion of character to a number is known as encoding, and the reverse is decoding.\n",
        "\n",
        "String literally surrounded by a single or double quotations like:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lqH5og0gJFm8",
        "outputId": "e92cb9d6-524c-4c3c-a92f-103a559df39c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "a = \"iNeuron\"\n",
        "print(a)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iNeuron\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1RCXneoOJFnJ"
      },
      "source": [
        "### Strings are arrays\n",
        "\n",
        "Like in other popular programming languages, strings in Python are arrays of bytes represents unicode characters.\n",
        "\n",
        "However, Python doesn't have the character as the datatype, single character is just simply a string with length of 1.\n",
        "\n",
        "We used square bracket to access elements from the string."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gNB6lImmJFnK",
        "outputId": "5dbe17be-a2b0-4396-a01d-eea70538baa2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#Get the character at position 4(Here, indexing starts from 0)\n",
        "a = \"iNeuron\"\n",
        "print(a[4])  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dfs-JIeSJFnS"
      },
      "source": [
        "### Slicing\n",
        "\n",
        "You can get the output upto certain range of characters by using the slicing index.\n",
        "\n",
        "Specify the start index and end index, which is separated by colon, to return a part of the string."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6T0u262JFnT",
        "outputId": "a74326eb-02d8-4bb5-a521-4ed94c2beefe"
      },
      "source": [
        "#To get the output from the position 3 to 6(not included)\n",
        "a = \"iNeuron\"\n",
        "print(a[3:6])\n",
        "print(\"-\" * 50)\n",
        "#To get the output by negative indexing from -6 position to -2 position.\n",
        "print(a[-6:-2])\n",
        "print(\"-\" * 50)\n",
        "#Get the results from position 2 to 6 but give result with the increment of 2.\n",
        "print(a[2:6:2])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "uro\n",
            "--------------------------------------------------\n",
            "Neur\n",
            "--------------------------------------------------\n",
            "er\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHou4XVkJFnc"
      },
      "source": [
        "### String Methods \n",
        "\n",
        "Python have set of built-in methods that you can use on strings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AaM_dDq7JFnd",
        "outputId": "cc397064-ed5f-43fe-a7b4-b65e6df20418",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#strip() will remove whitespace in the string from begining to the end.\n",
        "a = \" iNeuron \"\n",
        "print(a.split())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['iNeuron']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YiVPNNB2JFnm",
        "outputId": "c60724b2-ab81-4b64-ce6b-fa129e1ee371",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#lower() will lowercase the words which are upper in the sentences.\n",
        "a = \"iNeuron\"\n",
        "print(a.lower())\n",
        "\n",
        "#upper() will transform lowercase into upper.\n",
        "print(a.upper())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ineuron\n",
            "INEURON\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JxWGgRs6JFnv",
        "outputId": "00e70199-9020-45a2-b8c2-77f6d6a55933",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#replace() will work like replace one string with another string.\n",
        "a = \"iNeuron\"\n",
        "print(a.replace(\"i\", \"me\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "meNeuron\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p11MMZuUJFn4",
        "outputId": "a2262cc8-76c7-4cde-da79-d0952d12db3e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#split() will split the strings into substrings if it finds any instances of seprator.\n",
        "a = \"iNeuron\"\n",
        "print(a.split(\"o\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['iNeur', 'n']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3xSmTR6JFoA"
      },
      "source": [
        "### String Concatenation\n",
        "\n",
        "To concatenate or combine two strings by use of + operator."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_olPIBHJFoB",
        "outputId": "5e0ab97c-6436-4e56-a613-98dc22ceeb99",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "a = \"iNeuron\"\n",
        "b = \"Data\"\n",
        "print(a +\" \"+ b)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iNeuron Data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qo91WUzNJFpL"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AlPiVcIdJFpO"
      },
      "source": [
        "# Text Preprocessing\n",
        "\n",
        "Supose we have textual data available, we need to apply many of pre-processing steps to the data to transform those words into numerical features that work with machine learning algorithms.\n",
        "\n",
        "The pre-processing steps for the problem depend mainly on the domain and the problem itself.We don't need to apply all the steps for every problem.\n",
        "\n",
        "Here, we're going to see text preprocessing in Python. We'll use NLTK(Natural language toolkit) library here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K4JGBS1TJFpP"
      },
      "source": [
        "# import necessary libraries \n",
        "import nltk\n",
        "import string\n",
        "import re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FdD68bHhJFpT"
      },
      "source": [
        "### Text lowercase\n",
        "\n",
        "We do lowercase the text to reduce the size of the vocabulary of our text data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YoXbKGUcJFpT",
        "outputId": "8980ae33-f59d-4f25-fabd-7a6c78061cf8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def lowercase_text(text): \n",
        "    return text.lower() \n",
        "  \n",
        "input_str = \"Weather is too Cloudy.Possiblity of Rain is High,Today!!\"\n",
        "lowercase_text(input_str) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'weather is too cloudy.possiblity of rain is high,today!!'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kkx5nINIJFpa"
      },
      "source": [
        "### Remove numbers\n",
        "\n",
        "We should either remove the numbers or convert those numbers into textual representations.\n",
        "We use regular expressions(re) to remove the numbers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJxsrbYcJFpc",
        "outputId": "a6aefacb-c396-472d-ca86-3bbc8133b2e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# For Removing numbers \n",
        "def remove_num(text): \n",
        "    result = re.sub(r'\\d+', '', text) \n",
        "    return result \n",
        "  \n",
        "input_s = \"You bought 6 candies from shop, and 4 candies are in home.\"\n",
        "remove_num(input_s) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'You bought  candies from shop, and  candies are in home.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2dGfGX2JFpg"
      },
      "source": [
        "As we mentioned above,you can also convert the numbers into words. This could be done by using the inflect library."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VafdwS3cJFph",
        "outputId": "f6ede2f6-6dcd-40b0-dbb9-72356824f1c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# import the library \n",
        "import inflect \n",
        "q = inflect.engine() \n",
        "  \n",
        "# convert number into text \n",
        "def convert_num(text): \n",
        "    # split strings into list of texts \n",
        "    temp_string = text.split() \n",
        "    # initialise empty list \n",
        "    new_str = [] \n",
        "  \n",
        "    for word in temp_string: \n",
        "        # if text is a digit, convert the digit \n",
        "        # to numbers and append into the new_str list \n",
        "        if word.isdigit(): \n",
        "            temp = q.number_to_words(word) \n",
        "            new_str.append(temp) \n",
        "  \n",
        "        # append the texts as it is \n",
        "        else: \n",
        "            new_str.append(word) \n",
        "  \n",
        "    # join the texts of new_str to form a string \n",
        "    temp_str = ' '.join(new_str) \n",
        "    return temp_str \n",
        "  \n",
        "input_str = 'You bought 6 candies from shop, and 4 candies are in home.'\n",
        "convert_num(input_str)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'You bought six candies from shop, and four candies are in home.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "506D4NxOJFpk"
      },
      "source": [
        "### Remove Punctuation\n",
        "\n",
        "We remove punctuations because of that we don't have different form of the same word. If we don't remove punctuations, then been, been, and been! will be treated separately."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5B5eCetpJFpl",
        "outputId": "5e1d6c91-8e5f-4cda-935f-b81cf491316c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# let's remove punctuation \n",
        "def rem_punct(text): \n",
        "    translator = str.maketrans('', '', string.punctuation) \n",
        "    return text.translate(translator) \n",
        "  \n",
        "input_str = \"Hey, Are you excited??, After a week, we will be in Shimla!!!\"\n",
        "rem_punct(input_str) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hey Are you excited After a week we will be in Shimla'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99YJy-_ZJFpo"
      },
      "source": [
        "### Remove default stopwords:\n",
        "\n",
        "Stopwords are words that do not contribute to the meaning of the sentence. Hence, they can be safely removed without causing any change in the meaning of a sentence. The NLTK(Natural Language Toolkit) library has the set of stopwords and we can use these to remove stopwords from our text and return a list of word tokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zg0flBIrJFpp",
        "outputId": "db80011f-7277-476a-cfbd-3436f00a1e05",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "# importing nltk library\n",
        "from nltk.corpus import stopwords \n",
        "from nltk.tokenize import word_tokenize \n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "  \n",
        "# remove stopwords function \n",
        "def rem_stopwords(text): \n",
        "    stop_words = set(stopwords.words(\"english\")) \n",
        "    word_tokens = word_tokenize(text) \n",
        "    filtered_text = [word for word in word_tokens if word not in stop_words] \n",
        "    return filtered_text \n",
        "  \n",
        "ex_text = \"Data is the new oil. A.I is the last invention\"\n",
        "rem_stopwords(ex_text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Data', 'new', 'oil', '.', 'A.I', 'last', 'invention']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZMQOMO1JFps"
      },
      "source": [
        "### Stemming\n",
        "\n",
        "From Stemming we will process of getting the root form of a word. Root or Stem is the part to which inflextional affixes(like -ed, -ize, etc) are added. We would create the stem words by removing the prefix of suffix of a word. So, stemming a word may not result in actual words.\n",
        "\n",
        "For Example: Mangoes ---> Mango\n",
        "\n",
        "             Boys ---> Boy\n",
        "             \n",
        "             going ---> go\n",
        "             \n",
        "             \n",
        "If our sentences are not in tokens, then we need to convert it into tokens. After we converted strings of text into tokens, then we can convert those word tokens into their root form. These are the Porter stemmer, the snowball stemmer, and the Lancaster Stemmer. We usually use Porter stemmer among them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iwP4-kAgJFpt",
        "outputId": "7e7650de-1794-4051-d383-516feac7acbd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "#importing nltk's porter stemmer \n",
        "from nltk.stem.porter import PorterStemmer \n",
        "from nltk.tokenize import word_tokenize \n",
        "stem1 = PorterStemmer() \n",
        "  \n",
        "# stem words in the list of tokenised words \n",
        "def s_words(text): \n",
        "    word_tokens = word_tokenize(text) \n",
        "    stems = [stem1.stem(word) for word in word_tokens] \n",
        "    return stems \n",
        "  \n",
        "text = 'Data is the new revolution in the World, in a day one individual would generate terabytes of data.'\n",
        "s_words(text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['data',\n",
              " 'is',\n",
              " 'the',\n",
              " 'new',\n",
              " 'revolut',\n",
              " 'in',\n",
              " 'the',\n",
              " 'world',\n",
              " ',',\n",
              " 'in',\n",
              " 'a',\n",
              " 'day',\n",
              " 'one',\n",
              " 'individu',\n",
              " 'would',\n",
              " 'gener',\n",
              " 'terabyt',\n",
              " 'of',\n",
              " 'data',\n",
              " '.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3auiVuIJFpw"
      },
      "source": [
        "### Lemmatization\n",
        "\n",
        "As stemming, lemmatization do the same but the only difference is that lemmatization ensures that root word belongs to the language. Because of the use of lemmatization we will get the valid words. In NLTK(Natural language Toolkit), we use WordLemmatizer to get the lemmas of words. We also need to provide a context for the lemmatization.So, we added pos(parts-of-speech) as a parameter. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YavFewldJFpx",
        "outputId": "0bd98cf5-f60a-4008-dff0-23a09c4a8d7f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "from nltk.stem import wordnet \n",
        "from nltk.tokenize import word_tokenize \n",
        "lemma = wordnet.WordNetLemmatizer()\n",
        "nltk.download('wordnet')\n",
        "# lemmatize string \n",
        "def lemmatize_word(text): \n",
        "    word_tokens = word_tokenize(text) \n",
        "    # provide context i.e. part-of-speech(pos)\n",
        "    lemmas = [lemma.lemmatize(word, pos ='v') for word in word_tokens] \n",
        "    return lemmas \n",
        "  \n",
        "text = 'Data is the new revolution in the World, in a day one individual would generate terabytes of data.'\n",
        "lemmatize_word(text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Data',\n",
              " 'be',\n",
              " 'the',\n",
              " 'new',\n",
              " 'revolution',\n",
              " 'in',\n",
              " 'the',\n",
              " 'World',\n",
              " ',',\n",
              " 'in',\n",
              " 'a',\n",
              " 'day',\n",
              " 'one',\n",
              " 'individual',\n",
              " 'would',\n",
              " 'generate',\n",
              " 'terabytes',\n",
              " 'of',\n",
              " 'data',\n",
              " '.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vs9vYMcSQYu-",
        "outputId": "583d125c-d1a5-492e-c750-4b97599a3b30",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dt6ZYNpoJFp0"
      },
      "source": [
        "### Parts of Speech (POS) Tagging\n",
        "\n",
        "The pos(parts of speech) explain you how a word is used in a sentence. In the sentence, a word have different contexts and semantic meanings. The basic natural language processing(NLP) models like bag-of-words(bow) fails to identify these relation between the words. For that we use pos tagging to mark a word to its pos tag based on its context in the data. Pos is also used to extract rlationship between the words. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hCjm73RLJFp1",
        "outputId": "3818ce9c-c85c-4105-baf3-2060350687b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "# importing tokenize library\n",
        "from nltk.tokenize import word_tokenize \n",
        "from nltk import pos_tag \n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "  \n",
        "# convert text into word_tokens with their tags \n",
        "def pos_tagg(text): \n",
        "    word_tokens = word_tokenize(text) \n",
        "    return pos_tag(word_tokens) \n",
        "  \n",
        "pos_tagg('Are you afraid of something?') "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Are', 'NNP'),\n",
              " ('you', 'PRP'),\n",
              " ('afraid', 'IN'),\n",
              " ('of', 'IN'),\n",
              " ('something', 'NN'),\n",
              " ('?', '.')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JtdatdAsJFp4"
      },
      "source": [
        "In the above example NNP stands for Proper noun, PRP stands for personal noun, IN as Preposition. We can get all the details pos tags using the Penn Treebank tagset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Zv_rokfJFp5",
        "outputId": "04fbbb73-e494-40e1-8218-84c81d374ebe"
      },
      "source": [
        "# downloading the tagset  \n",
        "nltk.download('tagsets') \n",
        "  \n",
        "# extract information about the tag \n",
        "nltk.help.upenn_tagset('PRP')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package tagsets to\n",
            "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Unzipping help\\tagsets.zip.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "PRP: pronoun, personal\n",
            "    hers herself him himself hisself it itself me myself one oneself ours\n",
            "    ourselves ownself self she thee theirs them themselves they thou thy us\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRBPHRjSJFp8"
      },
      "source": [
        "### Chunking\n",
        "\n",
        "Chunking is the process of extracting phrases from the Unstructured text and give them more structure to it. We also called them shallow parsing.We can do it on top of pos tagging. It groups words into chunks mainly for noun phrases. chunking we do by using regular expression. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lzLmybEuJFp8",
        "outputId": "f88cb72f-e470-42e8-995b-6fd34d67e3bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "#importing libraries\n",
        "from nltk.tokenize import word_tokenize  \n",
        "from nltk import pos_tag \n",
        "  \n",
        "# here we define chunking function with text and regular \n",
        "# expressions representing grammar as parameter \n",
        "def chunking(text, grammar): \n",
        "    word_tokens = word_tokenize(text) \n",
        "  \n",
        "    # label words with pos \n",
        "    word_pos = pos_tag(word_tokens) \n",
        "  \n",
        "    # create chunk parser using grammar \n",
        "    chunkParser = nltk.RegexpParser(grammar) \n",
        "  \n",
        "    # test it on the list of word tokens with tagged pos \n",
        "    tree = chunkParser.parse(word_pos) \n",
        "      \n",
        "    for subtree in tree.subtrees(): \n",
        "        print(subtree) \n",
        "    #tree.draw() \n",
        "      \n",
        "sentence = 'the little red parrot is flying in the sky'\n",
        "grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
        "chunking(sentence, grammar) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(S\n",
            "  (NP the/DT little/JJ red/JJ parrot/NN)\n",
            "  is/VBZ\n",
            "  flying/VBG\n",
            "  in/IN\n",
            "  (NP the/DT sky/NN))\n",
            "(NP the/DT little/JJ red/JJ parrot/NN)\n",
            "(NP the/DT sky/NN)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYT9PqpwJFqA"
      },
      "source": [
        "In the above example, we defined the grammar by using the regular expression rule. This rule tells you that NP(noun phrase) chunk should be formed whenever the chunker find the optional determiner(DJ) followed by any no. of adjectives and then a NN(noun).\n",
        "\n",
        "Image after running above code.\n",
        "<img src=\".\\Images\\11.png\">\n",
        "\n",
        "Libraries like Spacy and TextBlob are best for chunking."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6faIGsRJFqB"
      },
      "source": [
        "### Named Entity Recognition\n",
        "\n",
        "It is used to extract information from unstructured text. It is used to classy the entities which is present in the text into categories like a person, organization, event, places, etc. This will give you a detail knowledge about the text and the relationship between the different entities."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QeA_JtIBJFqC",
        "outputId": "293ed047-f199-4841-8f5f-687e83ed15ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        }
      },
      "source": [
        "#Importing tokenization and chunk\n",
        "from nltk.tokenize import word_tokenize \n",
        "from nltk import pos_tag, ne_chunk \n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "  \n",
        "def ner(text): \n",
        "    # tokenize the text \n",
        "    word_tokens = word_tokenize(text) \n",
        "  \n",
        "    # pos tagging of words \n",
        "    word_pos = pos_tag(word_tokens) \n",
        "  \n",
        "    # tree of word entities \n",
        "    print(ne_chunk(word_pos)) \n",
        "  \n",
        "text = 'Brain Lara scored the highest 400 runs in a test match which played in between WI and England.'\n",
        "ner(text) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n",
            "(S\n",
            "  (PERSON Brain/NNP)\n",
            "  (PERSON Lara/NNP)\n",
            "  scored/VBD\n",
            "  the/DT\n",
            "  highest/JJS\n",
            "  400/CD\n",
            "  runs/NNS\n",
            "  in/IN\n",
            "  a/DT\n",
            "  test/NN\n",
            "  match/NN\n",
            "  which/WDT\n",
            "  played/VBD\n",
            "  in/IN\n",
            "  between/IN\n",
            "  (ORGANIZATION WI/NNP)\n",
            "  and/CC\n",
            "  (GPE England/NNP)\n",
            "  ./.)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F8QxWgIhJFqF"
      },
      "source": [
        "# Understanding Regex\n",
        "\n",
        "As you're a software developer, you have probably encountered regular expressions many times and got consufed many times with these daunting set of characters grouped together like this:\n",
        "\n",
        "<img src=\".\\Images\\12.png\">\n",
        "\n",
        "And you may wondered what this is all about?\n",
        "\n",
        "Regular Expressions(Regx or RegExp) are too useful in stepping up your algorithm game and this will make you a better problem solver. The structure of Regx can be intimidating at first, but it is very rewarding once you got all the patterns and implement them in your work properly.\n",
        "\n",
        "\n",
        "## What is RegEx and why is it important?\n",
        "\n",
        "A Regex or we called it as regular expression, it is a type of object will help you out to extract information from any string data by searching through text and find it out what you need.Whether it's punctuation, numbers, letters, or even white spaces, RegEx will allow you to check and match any of the character combination in strings.\n",
        "\n",
        "For example, suppose you need to match the format of a email addresses or security numbers. You can utilize RegEx to check the pattern inside the text strings and use it to replace another substring.\n",
        "\n",
        "For instance, a RegEx could tell the program to search for the specific text from the string and then to print out the output accordingly. Expressions can include Text matching, Repetition of words,Branching,pattern-composition.\n",
        "\n",
        "Python supports RegEx through libraries. In RegEx supports for various things like **Identifiers, Modifiers, and White Space.**\n",
        "<img src=\".\\Images\\13.png\">\n",
        "\n",
        "### RegEx Syntax\n",
        "\n",
        "    import re\n",
        "\n",
        "- *re* library in Python is used for string searching and manipulation.\n",
        "- We also used it frequently for web scraping.\n",
        "\n",
        "#### Example for w+ and ^ Expression\n",
        "\n",
        "- *^:* Here in this expression matches the start of a string.\n",
        "- *w+:* This expression matches for the alphanumeric characters from inside the string.\n",
        "\n",
        "Here, we will give one example of how you can use \"w+\" and \"^\" expressions in code. re.findall will cover in next parts,so just focus on the \"w+\" and \"^\" expression.\n",
        "\n",
        "Let's have an example \"iNeuron13, Data is a new fuel\", if we execute the code we will get \"iNeuron13\" as a result."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Ll7r_1RJFqG",
        "outputId": "75ac4d5e-daee-4cf5-c235-8220144b152a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import re\n",
        "sent = \"iNeuron13, Data is a new fuel\"\n",
        "r2 = re.findall(r\"^\\w+\",sent)\n",
        "print(r2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['iNeuron13']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCDIypANJFqJ"
      },
      "source": [
        "*Note:* If we remove '+' sign from \\w, the output will change and it'll give only first character of the first letter, i.e [i]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYONQsX4JFqJ"
      },
      "source": [
        "####  Example of \\s expression in re.split function\n",
        "\n",
        "- \"s:\" This expression we use for creating a space in the string.\n",
        "\n",
        "To understand better this expression we will use the split function in a simple example. In this example, we have to split each words using the \"re.split\" function and at the same time we have used \\s that allows to parse each word in the string seperately."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k8B4XnkyJFqL",
        "outputId": "4ce07e82-0e1b-44d9-e364-ba2a277d1255"
      },
      "source": [
        "import re\n",
        "\n",
        "print((re.split(r'\\s','We splited this sentence')))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['We', 'splited', 'this', 'sentence']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebbuYB4GJFqO"
      },
      "source": [
        "As we can see above we got the output ['We', 'splited', 'this', 'sentence'] but what if we remove ' \\ ' from '\\s', it will give result like remove 's' from the entire sentences. Let's see in below example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vOSeJ8heJFqP",
        "outputId": "4ca2a8bb-180d-4959-ae74-1790a25451de"
      },
      "source": [
        "import re\n",
        "\n",
        "print((re.split(r's','We splited this sentence')))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['We ', 'plited thi', ' ', 'entence']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ucqcu5sEJFqS"
      },
      "source": [
        "Similarly, there are series of regular expression in Python that you can use in various ways like  \\d,\\D,$,\\.,\\b, etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fsqRcZTBJFqS"
      },
      "source": [
        "## Use RegEx methods\n",
        "\n",
        "The \"re\" packages provide several methods to actually perform queries on an input string. We will see different methods which are\n",
        "\n",
        "    re.match()\n",
        "    re.search()\n",
        "    re.findall()\n",
        "    \n",
        "**Note:** Based on the RegEx, Python offers two different primitive operations. This match method checks for the match only at the begining of the string while search checks for a match anywhere in the string.\n",
        "\n",
        "### Using re.match()\n",
        "\n",
        "The match function is used to match the RegEx pattern to string with optional flag. Here, in this \"w+\" and \"\\W\" will match the words starting from \"i\" and thereafter ,anything which is not started with \"i\" is not identified. For checking match for each element in the list or string, we run the for loop."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9PwUHm80JFqT",
        "outputId": "c6be7161-c6e3-4a77-f90b-bd879c583671",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import re\n",
        "\n",
        "lists = ['icecream images', 'i immitated', 'inner peace']\n",
        "\n",
        "for i in lists:\n",
        "    q = re.match(\"(i\\w+)\\W(i\\w+)\", i)\n",
        "    \n",
        "    if q:\n",
        "        print((q.groups()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('icecream', 'images')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgpCmaYRJFqW"
      },
      "source": [
        "### Finding Pattern in the text(re.search())\n",
        "\n",
        "A RegEx is commonly used to search for a pattern in the text. This method takes a RegEx pattern and a string and searches that pattern with the string.\n",
        "\n",
        "For using re.search() function, you need to import re first. The search() function takes the \"pattern\" and \"text\" to scan from our given string and returns the match object when the pattern found or else not match."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-A3PluqJFqW",
        "outputId": "f8d99ecd-daed-4d8e-82af-4386c17f0afa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import re\n",
        "\n",
        "pattern = [\"playing\", \"iNeuron\"]\n",
        "text = \"Raju is playing outside.\"\n",
        "\n",
        "for p in pattern:\n",
        "    print(\"You're looking for '%s' in '%s'\" %(p, text), end = ' ')\n",
        "    \n",
        "    if re.search(p, text):\n",
        "        print('Found match!')\n",
        "        \n",
        "    else:\n",
        "        print(\"no match found!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "You're looking for 'playing' in 'Raju is playing outside.' Found match!\n",
            "You're looking for 'iNeuron' in 'Raju is playing outside.' no match found!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QpDqwUlYJFqc"
      },
      "source": [
        "In the Above example, we look for two literal strings \"playing\", \"iNeuron\" and in text string we had taken \"Raju is playing outside.\". For \"playing\" we got the match and in the output we got \"Found Match\", while for the word \"iNeuron\" we didn't got any match. So,we got no match found for that word."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Ymvv2F0JFqd"
      },
      "source": [
        "## Using re.findall() for text\n",
        "\n",
        "We use re.findall() module is when you wnat to iterate over the lines of the file, it'll do like list all the matches in one go. Here in a example, we would like to fetch email address from the list and we want to fetch all emails from the list, we use re.findall() method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oWqt1WHIJFqd",
        "outputId": "7d50adf6-d5b3-4c3f-92bc-0b27fed606c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "import re\n",
        "\n",
        "kgf = \"Gaurav@iNeuron.ai, Nilesh@iNeuron.ai, Jay@iNeuron.ai, Vikash@iNeuron.ai\"\n",
        "\n",
        "emails = re.findall(r'[\\w\\.-]+@[\\w\\.-]+', kgf)\n",
        "\n",
        "for e in emails:\n",
        "    print(e)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Gaurav@iNeuron.ai\n",
            "Nilesh@iNeuron.ai\n",
            "Jay@iNeuron.ai\n",
            "Vikash@iNeuron.ai\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5V2BOSMhJFqy"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Ar6ATbZJFs2"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}